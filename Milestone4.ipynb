{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109B Advanced Topics in Data Science, Final Project, Milestone 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 9 - Steve Robbins, Chad Tsang, and Ted Heuer\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Due Date: ** Wednesday, April 12th, 2017 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 4: Deep learning, due Wednesday, April 26, 2017\n",
    "\n",
    "For this milestone you will (finally) use deep learning to predict movie genres. You will train one small network from scratch on the posters only, and compare this one to a pre-trained network that you fine tune. [Here](https://keras.io/getting-started/faq/#how-can-i-use-pre-trained-models-in-keras) is a description of how to use pretrained models in Keras.\n",
    "\n",
    "You can try different architectures, initializations, parameter settings, optimization methods, etc. Be adventurous and explore deep learning! It can be fun to combine the features learned by the deep learning model with a SVM, or incorporate meta data into your deep learning model. \n",
    "\n",
    "**Note:** Be mindful of the longer training times for deep models. Not only for training time, but also for the parameter tuning efforts. You need time to develop a feel for the different parameters and which settings work, which normalization you want to use, which model architecture you choose, etc. \n",
    "\n",
    "It is great that we have GPUs via AWS to speed up the actual computation time, but you need to be mindful of your AWS credits. The GPU instances are not cheap and can accumulate costs rather quickly. Think about your model first and do some quick dry runs with a larger learning rate or large batch size on your local machine. \n",
    "\n",
    "The notebook to submit this week should at least include:\n",
    "\n",
    "- Complete description of the deep network you trained from scratch, including parameter settings, performance, features learned, etc. \n",
    "- Complete description of the pre-trained network that you fine tuned, including parameter settings, performance, features learned, etc. \n",
    "- Discussion of the results, how much improvement you gained with fine tuning, etc. \n",
    "- Discussion of at least one additional exploratory idea you pursued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links - Delete or cite these.\n",
    "https://keras.io/callbacks/\n",
    "\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "https://keras.io/getting-started/faq/#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory\n",
    "\n",
    "https://elitedatascience.com/keras-tutorial-deep-learning-in-python#step-4\n",
    "\n",
    "http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install keras \n",
    "#!pip install tensorflow\n",
    "#!pip install tensorflow.python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, split, and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_poster_data(image_size, source_size = 'w92', verbose = False):\n",
    "    # Loads the poster image data at the requested size, the assigned genre, and the movie id.\n",
    "    #\n",
    "    y_labels = pd.read_csv('y_labels_multiclass.csv')\n",
    "    image_path = './posters/' + source_size + '/'\n",
    "    posters = pd.DataFrame()\n",
    "    for movie in y_labels.iterrows():\n",
    "        row = movie[0]\n",
    "        movie_id = movie[1]['movie_id']\n",
    "        genre_id = int(movie[1]['genre_id'].replace('[', '').replace(']',''))\n",
    "        try:\n",
    "            image = misc.imread(image_path + str(movie_id) + '.jpg')\n",
    "            image_resize = img_to_array(misc.imresize(image, image_size))\n",
    "            if (image_resize.shape[2]==3):\n",
    "                posters = posters.append({'movie_id' : movie_id, \n",
    "                                          'genre_id' : genre_id,\n",
    "                                          'poster' : image_resize}, ignore_index = True)\n",
    "        except IOError:\n",
    "            if (verbose == True):\n",
    "                print('Unable to load poster for movie #', movie_id)\n",
    "    print('Loaded ', posters.shape[0], ' posters.')\n",
    "    return posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stratified_sampler(dataset, observations):\n",
    "    # Performs a stratified sample on the dataset and returns the number of observations \n",
    "    # requested.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #    dataset:  The dataframe to sample, observing class relationships.\n",
    "    #    observations:  The number of total target observations across all classes.\n",
    "    #\n",
    "    # Returns:\n",
    "    #    A pandas dataframe sampled from the dataset maintaining class relationships.\n",
    "    class_weights = dataset.groupby(\"genre_id\").agg(['count'])/len(dataset)\n",
    "    class_sample_counts = class_weights * observations\n",
    "    class_count = class_weights.shape[0]\n",
    "    sampled = pd.DataFrame()\n",
    "    for class_to_sample in class_sample_counts.iterrows():\n",
    "        class_name = class_to_sample[0]\n",
    "        desired_class_observations = class_to_sample[1][0]\n",
    "        sampled_obs = dataset[dataset[\"genre_id\"]==class_name].sample(int(desired_class_observations), replace=\"True\")\n",
    "        sampled = sampled.append(sampled_obs, ignore_index=True)\n",
    "    return sampled, class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reshape_and_normalize(data):\n",
    "    image_count = data.shape[0]\n",
    "    temp = np.ndarray(shape=(image_count, data[0].shape[0], data[0].shape[1], 3))\n",
    "\n",
    "    for index in range(0, image_count):\n",
    "        try:\n",
    "            temp[index] = data[index].reshape(data[0].shape[0], data[0].shape[1], 3)\n",
    "        except ValueError:\n",
    "            print(data[index].shape)\n",
    "    temp = temp.astype('float32')\n",
    "    temp /= 255.0\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_responses(data):\n",
    "    unique_responses = np.sort(data[\"genre_id\"].unique())\n",
    "    data[\"genre_id\"] = data[\"genre_id\"].replace(unique_responses, range(0,len(unique_responses)), inplace=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_split_prepare_data(train_observations, test_observations, image_size, sample = 'stratified'):\n",
    "    # Loads, splits, and prepares the data for use by a CNN model.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #    train_observations:  The dataframe to sample, observing class relationships.\n",
    "    #    test_observations:  The number of total target observations across all classes.\n",
    "    #    sample:  The sampling method, currently only supports 'stratified'\n",
    "    #\n",
    "    # Returns:\n",
    "    #    Nothing.\n",
    "    posters_data = load_poster_data(image_size)\n",
    "    posters = normalize_responses(posters_data)\n",
    "    \n",
    "    if (sample == 'stratified'):\n",
    "        train_sample, class_count_train = stratified_sampler(posters, train_observations)\n",
    "        test_sample, class_count_test = stratified_sampler(posters, test_observations)\n",
    "    else:\n",
    "        raise('Unsupported sample method : ', sample)\n",
    "         \n",
    "    x_train = train_sample[\"poster\"]\n",
    "    y_train = train_sample[\"genre_id\"]\n",
    "    x_test = test_sample[\"poster\"]\n",
    "    y_test = test_sample[\"genre_id\"]\n",
    "\n",
    "    img_rows = x_train[0].shape[0]\n",
    "    img_cols = x_train[0].shape[1]\n",
    "    print('Classes : ', class_count_train)\n",
    "        \n",
    "    x_train = reshape_and_normalize(x_train)\n",
    "    x_test = reshape_and_normalize(x_test)\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    # Convert response to one hot encoding\n",
    "    y_train = keras.utils.to_categorical(y_train, class_count_train)\n",
    "    y_test = keras.utils.to_categorical(y_test, class_count_test)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test), class_count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  3927  posters.\n",
      "Classes :  7\n",
      "x_train shape: (4996, 138, 92, 3)\n",
      "4996 train samples\n",
      "997 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), classes = load_split_prepare_data(train_observations = 5000, \n",
    "                                                                        test_observations = 1000, \n",
    "                                                                        image_size = (138,92), \n",
    "                                                                        sample='stratified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Convolutional Neural Net architecture, from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = 7\n",
    "final_activation_function = 'softmax'\n",
    "\n",
    "input_activation_function = 'relu'\n",
    "input_kernel_size = (5,5)\n",
    "input_shape = (138, 92, 3)\n",
    "pool_size = (3,3)\n",
    "\n",
    "hidden_activation_function = 'relu'\n",
    "hidden_kernel_size = (3,3)\n",
    "\n",
    "loss_method = 'categorical_crossentropy'\n",
    "optimizer = SGD(lr=0.1, momentum=0.9)\n",
    "eval_metric = 'accuracy'\n",
    "\n",
    "# smaller batch size means noisier gradient, but more updates per epoch\n",
    "batch_size = 256\n",
    "# number of iterations over the complete training data\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 134, 88, 16)       1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 44, 29, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 42, 27, 32)        4640      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4032)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                258112    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 264,423\n",
      "Trainable params: 264,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an empty network model\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Conv2D(16, kernel_size=input_kernel_size, activation=input_activation_function, input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Hidden Layer(s)\n",
    "model.add(Conv2D(32, kernel_size=hidden_kernel_size, activation=hidden_activation_function))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Adding another layer did not improve performance, perhaps because of the pooling on pooling.\n",
    "#model.add(Conv2D(48, kernel_size=hidden_kernel_size, activation=hidden_activation_function))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Classification layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=hidden_activation_function))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes, activation=final_activation_function))\n",
    "\n",
    "# Display the CNN.\n",
    "model.summary()\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(loss=loss_method, optimizer=optimizer, metrics=[eval_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4246 samples, validate on 750 samples\n",
      "Epoch 1/100\n",
      "4246/4246 [==============================] - 46s - loss: 1.6757 - acc: 0.2673 - val_loss: 4.3028 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "4246/4246 [==============================] - 54s - loss: 1.5105 - acc: 0.3667 - val_loss: 4.9602 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "4246/4246 [==============================] - 48s - loss: 1.4562 - acc: 0.4119 - val_loss: 4.4045 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "4246/4246 [==============================] - 47s - loss: 1.4631 - acc: 0.4131 - val_loss: 5.1297 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "4246/4246 [==============================] - 47s - loss: 1.4383 - acc: 0.4131 - val_loss: 6.4905 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "4246/4246 [==============================] - 47s - loss: 1.4573 - acc: 0.4114 - val_loss: 5.2273 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "4246/4246 [==============================] - 50s - loss: 1.4179 - acc: 0.4270 - val_loss: 5.7704 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "4246/4246 [==============================] - 55s - loss: 1.4026 - acc: 0.4345 - val_loss: 6.3438 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "4246/4246 [==============================] - 46s - loss: 1.3978 - acc: 0.4388 - val_loss: 5.9815 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "4246/4246 [==============================] - 54s - loss: 1.3350 - acc: 0.4659 - val_loss: 6.6594 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "4246/4246 [==============================] - 44s - loss: 1.3056 - acc: 0.4852 - val_loss: 6.9521 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "4246/4246 [==============================] - 44s - loss: 1.2937 - acc: 0.4903 - val_loss: 6.9775 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "4246/4246 [==============================] - 44s - loss: 1.2125 - acc: 0.5214 - val_loss: 7.3877 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "4246/4246 [==============================] - 44s - loss: 1.1558 - acc: 0.5462 - val_loss: 7.0124 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "4246/4246 [==============================] - 44s - loss: 1.1354 - acc: 0.5579 - val_loss: 7.4277 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "4246/4246 [==============================] - 45s - loss: 1.1339 - acc: 0.5624 - val_loss: 7.1523 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "4246/4246 [==============================] - 44s - loss: 1.0918 - acc: 0.5641 - val_loss: 6.9990 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "4246/4246 [==============================] - 44s - loss: 1.0865 - acc: 0.5714 - val_loss: 8.2570 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "4246/4246 [==============================] - 44s - loss: 1.0357 - acc: 0.5853 - val_loss: 7.2713 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.9594 - acc: 0.6227 - val_loss: 8.8157 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.9221 - acc: 0.6413 - val_loss: 9.1338 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.8713 - acc: 0.6686 - val_loss: 8.3293 - val_acc: 0.0013\n",
      "Epoch 23/100\n",
      "4246/4246 [==============================] - 46s - loss: 0.8745 - acc: 0.6597 - val_loss: 8.1003 - val_acc: 0.0013\n",
      "Epoch 24/100\n",
      "4246/4246 [==============================] - 59s - loss: 0.8266 - acc: 0.6825 - val_loss: 8.7974 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "4246/4246 [==============================] - 60s - loss: 0.7865 - acc: 0.7007 - val_loss: 9.9416 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "4246/4246 [==============================] - 47s - loss: 0.7819 - acc: 0.6955 - val_loss: 9.1421 - val_acc: 0.0013\n",
      "Epoch 27/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.7545 - acc: 0.7063 - val_loss: 9.5400 - val_acc: 0.0013\n",
      "Epoch 28/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.6890 - acc: 0.7386 - val_loss: 9.9352 - val_acc: 0.0013\n",
      "Epoch 29/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.6740 - acc: 0.7416 - val_loss: 9.5078 - val_acc: 0.0013\n",
      "Epoch 30/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.6945 - acc: 0.7313 - val_loss: 10.2365 - val_acc: 0.0013\n",
      "Epoch 31/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.6614 - acc: 0.7456 - val_loss: 10.3629 - val_acc: 0.0013\n",
      "Epoch 32/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.6368 - acc: 0.7555 - val_loss: 10.4359 - val_acc: 0.0013\n",
      "Epoch 33/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.6021 - acc: 0.7697 - val_loss: 10.1788 - val_acc: 0.0013\n",
      "Epoch 34/100\n",
      "4246/4246 [==============================] - 48s - loss: 0.6146 - acc: 0.7624 - val_loss: 10.1700 - val_acc: 0.0027\n",
      "Epoch 35/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.5633 - acc: 0.7781 - val_loss: 10.8770 - val_acc: 0.0027\n",
      "Epoch 36/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.6373 - acc: 0.7591 - val_loss: 10.7587 - val_acc: 0.0027\n",
      "Epoch 37/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.6125 - acc: 0.7633 - val_loss: 10.3362 - val_acc: 0.0027\n",
      "Epoch 38/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.5395 - acc: 0.7897 - val_loss: 11.6691 - val_acc: 0.0027\n",
      "Epoch 39/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.5325 - acc: 0.7977 - val_loss: 11.0001 - val_acc: 0.0027\n",
      "Epoch 40/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.5255 - acc: 0.7960 - val_loss: 11.4931 - val_acc: 0.0027\n",
      "Epoch 41/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.5135 - acc: 0.7984 - val_loss: 10.8765 - val_acc: 0.0027\n",
      "Epoch 42/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.4996 - acc: 0.8064 - val_loss: 12.0326 - val_acc: 0.0027\n",
      "Epoch 43/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4942 - acc: 0.8146 - val_loss: 10.7194 - val_acc: 0.0027\n",
      "Epoch 44/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4992 - acc: 0.8055 - val_loss: 10.7122 - val_acc: 0.0027\n",
      "Epoch 45/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.4613 - acc: 0.8245 - val_loss: 11.7362 - val_acc: 0.0027\n",
      "Epoch 46/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4479 - acc: 0.8217 - val_loss: 11.9361 - val_acc: 0.0027\n",
      "Epoch 47/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4399 - acc: 0.8274 - val_loss: 12.2649 - val_acc: 0.0027\n",
      "Epoch 48/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4455 - acc: 0.8318 - val_loss: 11.5312 - val_acc: 0.0027\n",
      "Epoch 49/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4420 - acc: 0.8297 - val_loss: 11.6912 - val_acc: 0.0027\n",
      "Epoch 50/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4675 - acc: 0.8165 - val_loss: 12.1090 - val_acc: 0.0027\n",
      "Epoch 51/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4253 - acc: 0.8363 - val_loss: 11.6884 - val_acc: 0.0027\n",
      "Epoch 52/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4272 - acc: 0.8302 - val_loss: 12.0646 - val_acc: 0.0027\n",
      "Epoch 53/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4189 - acc: 0.8370 - val_loss: 12.5649 - val_acc: 0.0027\n",
      "Epoch 54/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4187 - acc: 0.8389 - val_loss: 11.9353 - val_acc: 0.0027\n",
      "Epoch 55/100\n",
      "4246/4246 [==============================] - 44s - loss: 0.4801 - acc: 0.8179 - val_loss: 11.1571 - val_acc: 0.0027\n",
      "Epoch 56/100\n",
      "4246/4246 [==============================] - 58s - loss: 0.4113 - acc: 0.8328 - val_loss: 12.2831 - val_acc: 0.0027\n",
      "Epoch 57/100\n",
      "4246/4246 [==============================] - 59s - loss: 0.3872 - acc: 0.8533 - val_loss: 12.5366 - val_acc: 0.0027\n",
      "Epoch 58/100\n",
      "4246/4246 [==============================] - 60s - loss: 0.4042 - acc: 0.8453 - val_loss: 11.3829 - val_acc: 0.0027\n",
      "Epoch 59/100\n",
      "4246/4246 [==============================] - 60s - loss: 0.3815 - acc: 0.8502 - val_loss: 12.5355 - val_acc: 0.0027\n",
      "Epoch 60/100\n",
      "4246/4246 [==============================] - 60s - loss: 0.3654 - acc: 0.8646 - val_loss: 12.3291 - val_acc: 0.0027\n",
      "Epoch 61/100\n",
      "4246/4246 [==============================] - 46s - loss: 0.3815 - acc: 0.8542 - val_loss: 11.5790 - val_acc: 0.0027\n",
      "Epoch 62/100\n",
      "4246/4246 [==============================] - 46s - loss: 0.3838 - acc: 0.8535 - val_loss: 11.8806 - val_acc: 0.0027\n",
      "Epoch 63/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.3814 - acc: 0.8476 - val_loss: 12.5288 - val_acc: 0.0027\n",
      "Epoch 64/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.3678 - acc: 0.8512 - val_loss: 12.6691 - val_acc: 0.0013\n",
      "Epoch 65/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.4273 - acc: 0.8391 - val_loss: 12.9951 - val_acc: 0.0027\n",
      "Epoch 66/100\n",
      "4246/4246 [==============================] - 51s - loss: 0.4166 - acc: 0.8391 - val_loss: 11.8185 - val_acc: 0.0027\n",
      "Epoch 67/100\n",
      "4246/4246 [==============================] - 53s - loss: 0.3602 - acc: 0.8618 - val_loss: 12.6532 - val_acc: 0.0027\n",
      "Epoch 68/100\n",
      "4246/4246 [==============================] - 50s - loss: 0.3851 - acc: 0.8563 - val_loss: 12.4229 - val_acc: 0.0027\n",
      "Epoch 69/100\n",
      "4246/4246 [==============================] - 56s - loss: 0.3486 - acc: 0.8625 - val_loss: 12.5345 - val_acc: 0.0027\n",
      "Epoch 70/100\n",
      "4246/4246 [==============================] - 50s - loss: 0.3670 - acc: 0.8599 - val_loss: 11.7629 - val_acc: 0.0027\n",
      "Epoch 71/100\n",
      "4246/4246 [==============================] - 52s - loss: 0.3881 - acc: 0.8493 - val_loss: 12.4226 - val_acc: 0.0027\n",
      "Epoch 72/100\n",
      "4246/4246 [==============================] - 53s - loss: 0.3629 - acc: 0.8592 - val_loss: 12.7048 - val_acc: 0.0027\n",
      "Epoch 73/100\n",
      "4246/4246 [==============================] - 58s - loss: 0.3503 - acc: 0.8672 - val_loss: 12.7433 - val_acc: 0.0027\n",
      "Epoch 74/100\n",
      "4246/4246 [==============================] - 50s - loss: 0.3134 - acc: 0.8745 - val_loss: 12.9767 - val_acc: 0.0027\n",
      "Epoch 75/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.3232 - acc: 0.8752 - val_loss: 12.6336 - val_acc: 0.0027\n",
      "Epoch 76/100\n",
      "4246/4246 [==============================] - 46s - loss: 0.3115 - acc: 0.8804 - val_loss: 12.9262 - val_acc: 0.0027\n",
      "Epoch 77/100\n",
      "4246/4246 [==============================] - 55s - loss: 0.3242 - acc: 0.8693 - val_loss: 13.3527 - val_acc: 0.0027\n",
      "Epoch 78/100\n",
      "4246/4246 [==============================] - 53s - loss: 0.3588 - acc: 0.8632 - val_loss: 12.4893 - val_acc: 0.0027\n",
      "Epoch 79/100\n",
      "4246/4246 [==============================] - 57s - loss: 0.3284 - acc: 0.8707 - val_loss: 12.2666 - val_acc: 0.0027\n",
      "Epoch 80/100\n",
      "4246/4246 [==============================] - 62s - loss: 0.3133 - acc: 0.8829 - val_loss: 13.0471 - val_acc: 0.0027\n",
      "Epoch 81/100\n",
      "4246/4246 [==============================] - 55s - loss: 0.3121 - acc: 0.8808 - val_loss: 13.0927 - val_acc: 0.0027\n",
      "Epoch 82/100\n",
      "4246/4246 [==============================] - 45s - loss: 0.2900 - acc: 0.8834 - val_loss: 13.2421 - val_acc: 0.0027\n",
      "Epoch 83/100\n",
      "4246/4246 [==============================] - 61s - loss: 0.2876 - acc: 0.8844 - val_loss: 13.1302 - val_acc: 0.0027\n",
      "Epoch 84/100\n",
      "4246/4246 [==============================] - 53s - loss: 0.3342 - acc: 0.8669 - val_loss: 13.0372 - val_acc: 0.0027\n",
      "Epoch 85/100\n",
      "4246/4246 [==============================] - 48s - loss: 0.3523 - acc: 0.8606 - val_loss: 12.5567 - val_acc: 0.0027\n",
      "Epoch 86/100\n",
      "4246/4246 [==============================] - 48s - loss: 0.3147 - acc: 0.8761 - val_loss: 13.0748 - val_acc: 0.0027\n",
      "Epoch 87/100\n",
      "4246/4246 [==============================] - 51s - loss: 0.3267 - acc: 0.8764 - val_loss: 12.9838 - val_acc: 0.0027\n",
      "Epoch 88/100\n",
      "4246/4246 [==============================] - 53s - loss: 0.3158 - acc: 0.8806 - val_loss: 12.2616 - val_acc: 0.0027\n",
      "Epoch 89/100\n",
      "4246/4246 [==============================] - 49s - loss: 0.3361 - acc: 0.8686 - val_loss: 12.7635 - val_acc: 0.0027\n",
      "Epoch 90/100\n",
      "4246/4246 [==============================] - 47s - loss: 0.3157 - acc: 0.8829 - val_loss: 13.4482 - val_acc: 0.0027\n",
      "Epoch 91/100\n",
      "4246/4246 [==============================] - 49s - loss: 0.3146 - acc: 0.8792 - val_loss: 13.2828 - val_acc: 0.0027\n",
      "Epoch 92/100\n",
      "4246/4246 [==============================] - 50s - loss: 0.3090 - acc: 0.8808 - val_loss: 12.8070 - val_acc: 0.0027\n",
      "Epoch 93/100\n",
      "4246/4246 [==============================] - 55s - loss: 0.2899 - acc: 0.8917 - val_loss: 12.8923 - val_acc: 0.0027\n",
      "Epoch 94/100\n",
      "4246/4246 [==============================] - 49s - loss: 0.2865 - acc: 0.8907 - val_loss: 13.2762 - val_acc: 0.0027\n",
      "Epoch 95/100\n",
      "4246/4246 [==============================] - 47s - loss: 0.3099 - acc: 0.8813 - val_loss: 12.6358 - val_acc: 0.0027\n",
      "Epoch 96/100\n",
      "4246/4246 [==============================] - 47s - loss: 0.2824 - acc: 0.8954 - val_loss: 12.2538 - val_acc: 0.0027\n",
      "Epoch 97/100\n",
      "4246/4246 [==============================] - 50s - loss: 0.2813 - acc: 0.8928 - val_loss: 13.8606 - val_acc: 0.0027\n",
      "Epoch 98/100\n",
      "4246/4246 [==============================] - 48s - loss: 0.2894 - acc: 0.8947 - val_loss: 13.0086 - val_acc: 0.0027\n",
      "Epoch 99/100\n",
      "4246/4246 [==============================] - 46s - loss: 0.2797 - acc: 0.8931 - val_loss: 12.7952 - val_acc: 0.0027\n",
      "Epoch 100/100\n",
      "4246/4246 [==============================] - 46s - loss: 0.3019 - acc: 0.8829 - val_loss: 13.1301 - val_acc: 0.0027\n",
      "Test loss: 2.79474070701\n",
      "Test accuracy: 0.692076228686\n"
     ]
    }
   ],
   "source": [
    "# The actual training of the CNN using the parameters and model previously specified.\n",
    "# The validation set is a split of the stratified sampled training data.\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split = 0.15)\n",
    "\n",
    "# Evaluate the performance on the unused testing set.\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('tuned_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x115396810>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAFkCAYAAAA9h3LKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACzzASPfVWOUlXRfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115309f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here is a visualization of the training process\n",
    "# typically we gain a lot in the beginning and then\n",
    "# training slows down\n",
    "plt.plot(history.history['acc'])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tune an existing CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3a1eaca7d7ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# add the model on top of the convolutional base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel_tune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# set the first 25 layers (up to the last conv block)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = 'tuned_weights.h5'\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# build the VGG16 network\n",
    "model_tune = applications.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=input_shape))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "#top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "model_tune.add(top_model)\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model_tune.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model_tune.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fine-tune the model\n",
    "model_tune.fit_generator(\n",
    "    x_train, y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
