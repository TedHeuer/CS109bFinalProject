{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109B Advanced Topics in Data Science, Final Project, Milestone 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 9 - Steve Robbins, Chad Tsang, and Ted Heuer\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Due Date: ** Wednesday, April 12th, 2017 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 4: Deep learning, due Wednesday, April 26, 2017\n",
    "\n",
    "For this milestone you will (finally) use deep learning to predict movie genres. You will train one small network from scratch on the posters only, and compare this one to a pre-trained network that you fine tune. [Here](https://keras.io/getting-started/faq/#how-can-i-use-pre-trained-models-in-keras) is a description of how to use pretrained models in Keras.\n",
    "\n",
    "You can try different architectures, initializations, parameter settings, optimization methods, etc. Be adventurous and explore deep learning! It can be fun to combine the features learned by the deep learning model with a SVM, or incorporate meta data into your deep learning model. \n",
    "\n",
    "**Note:** Be mindful of the longer training times for deep models. Not only for training time, but also for the parameter tuning efforts. You need time to develop a feel for the different parameters and which settings work, which normalization you want to use, which model architecture you choose, etc. \n",
    "\n",
    "It is great that we have GPUs via AWS to speed up the actual computation time, but you need to be mindful of your AWS credits. The GPU instances are not cheap and can accumulate costs rather quickly. Think about your model first and do some quick dry runs with a larger learning rate or large batch size on your local machine. \n",
    "\n",
    "The notebook to submit this week should at least include:\n",
    "\n",
    "- Complete description of the deep network you trained from scratch, including parameter settings, performance, features learned, etc. \n",
    "- Complete description of the pre-trained network that you fine tuned, including parameter settings, performance, features learned, etc. \n",
    "- Discussion of the results, how much improvement you gained with fine tuning, etc. \n",
    "- Discussion of at least one additional exploratory idea you pursued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links - Delete or cite these.\n",
    "https://keras.io/callbacks/\n",
    "\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "https://keras.io/getting-started/faq/#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory\n",
    "\n",
    "https://elitedatascience.com/keras-tutorial-deep-learning-in-python#step-4\n",
    "\n",
    "http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install keras \n",
    "#!pip install tensorflow\n",
    "#!pip install tensorflow.python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, split, and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_poster_data(image_size, source_size = 'w92', verbose = False):\n",
    "    # Loads the poster image data at the requested size, the assigned genre, and the movie id.\n",
    "    #\n",
    "    y_labels = pd.read_csv('y_labels_multiclass.csv')\n",
    "    image_path = './posters/' + source_size + '/'\n",
    "    posters = pd.DataFrame()\n",
    "    for movie in y_labels.iterrows():\n",
    "        row = movie[0]\n",
    "        movie_id = movie[1]['movie_id']\n",
    "        genre_id = int(movie[1]['genre_id'].replace('[', '').replace(']',''))\n",
    "        try:\n",
    "            image = misc.imread(image_path + str(movie_id) + '.jpg')\n",
    "            image_resize = img_to_array(misc.imresize(image, image_size))\n",
    "            if (image_resize.shape[2]==3):\n",
    "                posters = posters.append({'movie_id' : movie_id, \n",
    "                                          'genre_id' : genre_id,\n",
    "                                          'poster' : image_resize}, ignore_index = True)\n",
    "        except IOError:\n",
    "            if (verbose == True):\n",
    "                print('Unable to load poster for movie #', movie_id)\n",
    "    print('Loaded ', posters.shape[0], ' posters.')\n",
    "    return posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stratified_sampler(dataset, observations):\n",
    "    # Performs a stratified sample on the dataset and returns the number of observations \n",
    "    # requested.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #    dataset:  The dataframe to sample, observing class relationships.\n",
    "    #    observations:  The number of total target observations across all classes.\n",
    "    #\n",
    "    # Returns:\n",
    "    #    A pandas dataframe sampled from the dataset maintaining class relationships.\n",
    "    class_weights = dataset.groupby(\"genre_id\").agg(['count'])/len(dataset)\n",
    "    class_sample_counts = class_weights * observations\n",
    "    class_count = class_weights.shape[0]\n",
    "    sampled = pd.DataFrame()\n",
    "    for class_to_sample in class_sample_counts.iterrows():\n",
    "        class_name = class_to_sample[0]\n",
    "        desired_class_observations = class_to_sample[1][0]\n",
    "        sampled_obs = dataset[dataset[\"genre_id\"]==class_name].sample(int(desired_class_observations), replace=\"True\")\n",
    "        sampled = sampled.append(sampled_obs, ignore_index=True)\n",
    "    return sampled, class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reshape_and_normalize(data):\n",
    "    image_count = data.shape[0]\n",
    "    temp = np.ndarray(shape=(image_count, data[0].shape[0], data[0].shape[1], 3))\n",
    "\n",
    "    for index in range(0, image_count):\n",
    "        try:\n",
    "            temp[index] = data[index].reshape(data[0].shape[0], data[0].shape[1], 3)\n",
    "        except ValueError:\n",
    "            print(data[index].shape)\n",
    "    temp = temp.astype('float32')\n",
    "    temp /= 255.0\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_responses(data):\n",
    "    unique_responses = np.sort(data[\"genre_id\"].unique())\n",
    "    data[\"genre_id\"] = data[\"genre_id\"].replace(unique_responses, range(0,len(unique_responses)), inplace=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_split_prepare_data(train_observations, test_observations, image_size, sample = 'stratified'):\n",
    "    # Loads, splits, and prepares the data for use by a CNN model.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #    train_observations:  The dataframe to sample, observing class relationships.\n",
    "    #    test_observations:  The number of total target observations across all classes.\n",
    "    #    sample:  The sampling method, currently only supports 'stratified'\n",
    "    #\n",
    "    # Returns:\n",
    "    #    Nothing.\n",
    "    posters_data = load_poster_data(image_size)\n",
    "    posters = normalize_responses(posters_data)\n",
    "    \n",
    "    if (sample == 'stratified'):\n",
    "        train_sample, class_count_train = stratified_sampler(posters, train_observations)\n",
    "        test_sample, class_count_test = stratified_sampler(posters, test_observations)\n",
    "    else:\n",
    "        raise('Unsupported sample method : ', sample)\n",
    "         \n",
    "    x_train = train_sample[\"poster\"]\n",
    "    y_train = train_sample[\"genre_id\"]\n",
    "    x_test = test_sample[\"poster\"]\n",
    "    y_test = test_sample[\"genre_id\"]\n",
    "\n",
    "    img_rows = x_train[0].shape[0]\n",
    "    img_cols = x_train[0].shape[1]\n",
    "    print('Classes : ', class_count_train)\n",
    "        \n",
    "    x_train = reshape_and_normalize(x_train)\n",
    "    x_test = reshape_and_normalize(x_test)\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    # Convert response to one hot encoding\n",
    "    y_train = keras.utils.to_categorical(y_train, class_count_train)\n",
    "    y_test = keras.utils.to_categorical(y_test, class_count_test)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test), class_count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  3927  posters.\n",
      "Classes :  7\n",
      "x_train shape: (4996, 138, 92, 3)\n",
      "4996 train samples\n",
      "997 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), classes = load_split_prepare_data(train_observations = 5000, \n",
    "                                                                        test_observations = 1000, \n",
    "                                                                        image_size = (138,92), \n",
    "                                                                        sample='stratified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Convolutional Neural Net architecture, from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = 7\n",
    "final_activation_function = 'softmax'\n",
    "\n",
    "input_activation_function = 'relu'\n",
    "input_kernel_size = (5,5)\n",
    "input_shape = (138, 92, 3)\n",
    "pool_size = (3,3)\n",
    "\n",
    "hidden_activation_function = 'relu'\n",
    "hidden_kernel_size = (3,3)\n",
    "\n",
    "loss_method = 'categorical_crossentropy'\n",
    "optimizer = SGD(lr=0.1, momentum=0.9)\n",
    "eval_metric = 'accuracy'\n",
    "\n",
    "# smaller batch size means noisier gradient, but more updates per epoch\n",
    "batch_size = 256\n",
    "# number of iterations over the complete training data\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 134, 88, 16)       1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 44, 29, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 42, 27, 32)        4640      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4032)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                258112    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 264,423\n",
      "Trainable params: 264,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an empty network model\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Conv2D(16, kernel_size=input_kernel_size, activation=input_activation_function, input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Hidden Layer(s)\n",
    "model.add(Conv2D(32, kernel_size=hidden_kernel_size, activation=hidden_activation_function))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Adding another layer did not improve performance, perhaps because of the pooling on pooling.\n",
    "#model.add(Conv2D(48, kernel_size=hidden_kernel_size, activation=hidden_activation_function))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Classification layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=hidden_activation_function))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes, activation=final_activation_function))\n",
    "\n",
    "# Display the CNN.\n",
    "model.summary()\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(loss=loss_method, optimizer=optimizer, metrics=[eval_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4246 samples, validate on 750 samples\n",
      "Epoch 1/200\n",
      "4246/4246 [==============================] - 47s - loss: 1.6710 - acc: 0.2862 - val_loss: 4.6371 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.5130 - acc: 0.3754 - val_loss: 5.0491 - val_acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "4246/4246 [==============================] - 45s - loss: 1.5057 - acc: 0.3620 - val_loss: 5.2226 - val_acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "4246/4246 [==============================] - 45s - loss: 1.4251 - acc: 0.4091 - val_loss: 6.1261 - val_acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "4246/4246 [==============================] - 44s - loss: 1.3611 - acc: 0.4277 - val_loss: 5.1586 - val_acc: 0.0000e+00\n",
      "Epoch 6/200\n",
      "4246/4246 [==============================] - 44s - loss: 1.3344 - acc: 0.4477 - val_loss: 6.5343 - val_acc: 0.0000e+00\n",
      "Epoch 7/200\n",
      "4246/4246 [==============================] - 44s - loss: 1.2504 - acc: 0.4873 - val_loss: 7.3032 - val_acc: 0.0000e+00\n",
      "Epoch 8/200\n",
      "4246/4246 [==============================] - 56s - loss: 1.2112 - acc: 0.5064 - val_loss: 6.5253 - val_acc: 0.0000e+00\n",
      "Epoch 9/200\n",
      "4246/4246 [==============================] - 45s - loss: 1.1159 - acc: 0.5518 - val_loss: 7.9559 - val_acc: 0.0000e+00\n",
      "Epoch 10/200\n",
      "4246/4246 [==============================] - 44s - loss: 1.0848 - acc: 0.5584 - val_loss: 6.8345 - val_acc: 0.0027\n",
      "Epoch 11/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.9796 - acc: 0.6102 - val_loss: 7.7287 - val_acc: 0.0027\n",
      "Epoch 12/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.9342 - acc: 0.6361 - val_loss: 7.2427 - val_acc: 0.0027\n",
      "Epoch 13/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.9048 - acc: 0.6491 - val_loss: 8.9474 - val_acc: 0.0027\n",
      "Epoch 14/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.7865 - acc: 0.6879 - val_loss: 8.7438 - val_acc: 0.0027\n",
      "Epoch 15/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.7528 - acc: 0.7101 - val_loss: 9.4689 - val_acc: 0.0027\n",
      "Epoch 16/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.7236 - acc: 0.7176 - val_loss: 9.4193 - val_acc: 0.0027\n",
      "Epoch 17/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.7101 - acc: 0.7204 - val_loss: 10.2099 - val_acc: 0.0027\n",
      "Epoch 18/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.6340 - acc: 0.7492 - val_loss: 8.9104 - val_acc: 0.0027\n",
      "Epoch 19/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.6251 - acc: 0.7602 - val_loss: 8.5795 - val_acc: 0.0027\n",
      "Epoch 20/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.5736 - acc: 0.7715 - val_loss: 10.4932 - val_acc: 0.0027\n",
      "Epoch 21/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.5707 - acc: 0.7810 - val_loss: 9.7364 - val_acc: 0.0027\n",
      "Epoch 22/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.5609 - acc: 0.7786 - val_loss: 9.4854 - val_acc: 0.0040\n",
      "Epoch 23/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.5073 - acc: 0.8038 - val_loss: 10.0042 - val_acc: 0.0040\n",
      "Epoch 24/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.4871 - acc: 0.8125 - val_loss: 9.7003 - val_acc: 0.0040\n",
      "Epoch 25/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.5251 - acc: 0.8003 - val_loss: 10.0434 - val_acc: 0.0040\n",
      "Epoch 26/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.5005 - acc: 0.8062 - val_loss: 10.5111 - val_acc: 0.0040\n",
      "Epoch 27/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.4437 - acc: 0.8269 - val_loss: 10.9676 - val_acc: 0.0040\n",
      "Epoch 28/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.4717 - acc: 0.8123 - val_loss: 10.4518 - val_acc: 0.0040\n",
      "Epoch 29/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.4121 - acc: 0.8453 - val_loss: 11.1354 - val_acc: 0.0040\n",
      "Epoch 30/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.4238 - acc: 0.8446 - val_loss: 10.9979 - val_acc: 0.0040\n",
      "Epoch 31/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.4029 - acc: 0.8431 - val_loss: 11.1972 - val_acc: 0.0040\n",
      "Epoch 32/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.3974 - acc: 0.8431 - val_loss: 10.9410 - val_acc: 0.0040\n",
      "Epoch 33/200\n",
      "4246/4246 [==============================] - 54s - loss: 0.4097 - acc: 0.8436 - val_loss: 11.2344 - val_acc: 0.0040\n",
      "Epoch 34/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.3995 - acc: 0.8469 - val_loss: 10.6216 - val_acc: 0.0040\n",
      "Epoch 35/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.3725 - acc: 0.8585 - val_loss: 11.3429 - val_acc: 0.0040\n",
      "Epoch 36/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.4319 - acc: 0.8391 - val_loss: 10.5825 - val_acc: 0.0027\n",
      "Epoch 37/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.4185 - acc: 0.8398 - val_loss: 11.2579 - val_acc: 0.0040\n",
      "Epoch 38/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.3522 - acc: 0.8669 - val_loss: 11.1940 - val_acc: 0.0040\n",
      "Epoch 39/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.3442 - acc: 0.8683 - val_loss: 11.6164 - val_acc: 0.0040\n",
      "Epoch 40/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.3385 - acc: 0.8683 - val_loss: 11.4355 - val_acc: 0.0040\n",
      "Epoch 41/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.3013 - acc: 0.8853 - val_loss: 12.0043 - val_acc: 0.0040\n",
      "Epoch 42/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.3229 - acc: 0.8764 - val_loss: 11.1187 - val_acc: 0.0040\n",
      "Epoch 43/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.2954 - acc: 0.8914 - val_loss: 11.6937 - val_acc: 0.0040\n",
      "Epoch 44/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.3018 - acc: 0.8935 - val_loss: 11.9744 - val_acc: 0.0040\n",
      "Epoch 45/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.2865 - acc: 0.8900 - val_loss: 12.6906 - val_acc: 0.0040\n",
      "Epoch 46/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.3057 - acc: 0.8898 - val_loss: 11.9069 - val_acc: 0.0040\n",
      "Epoch 47/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.3061 - acc: 0.8820 - val_loss: 11.7134 - val_acc: 0.0040\n",
      "Epoch 48/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.3177 - acc: 0.8797 - val_loss: 11.2174 - val_acc: 0.0040\n",
      "Epoch 49/200\n",
      "4246/4246 [==============================] - 49s - loss: 0.2917 - acc: 0.8855 - val_loss: 11.6810 - val_acc: 0.0040\n",
      "Epoch 50/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.3036 - acc: 0.8900 - val_loss: 11.4748 - val_acc: 0.0027\n",
      "Epoch 51/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.3811 - acc: 0.8662 - val_loss: 10.8608 - val_acc: 0.0040\n",
      "Epoch 52/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.3372 - acc: 0.8756 - val_loss: 12.1870 - val_acc: 0.0040\n",
      "Epoch 53/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.3288 - acc: 0.8792 - val_loss: 11.0298 - val_acc: 0.0040\n",
      "Epoch 54/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.3326 - acc: 0.8818 - val_loss: 12.0230 - val_acc: 0.0040\n",
      "Epoch 55/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.3301 - acc: 0.8825 - val_loss: 11.7192 - val_acc: 0.0040\n",
      "Epoch 56/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.3378 - acc: 0.8768 - val_loss: 12.0165 - val_acc: 0.0040\n",
      "Epoch 57/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.3413 - acc: 0.8700 - val_loss: 11.6368 - val_acc: 0.0040\n",
      "Epoch 58/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.3296 - acc: 0.8825 - val_loss: 11.3493 - val_acc: 0.0040\n",
      "Epoch 59/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2932 - acc: 0.8895 - val_loss: 12.0953 - val_acc: 0.0040\n",
      "Epoch 60/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2700 - acc: 0.9056 - val_loss: 12.2489 - val_acc: 0.0040\n",
      "Epoch 61/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2678 - acc: 0.8961 - val_loss: 12.2971 - val_acc: 0.0040\n",
      "Epoch 62/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2621 - acc: 0.9079 - val_loss: 12.5112 - val_acc: 0.0040\n",
      "Epoch 63/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2518 - acc: 0.9098 - val_loss: 12.2629 - val_acc: 0.0040\n",
      "Epoch 64/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2706 - acc: 0.8990 - val_loss: 12.2297 - val_acc: 0.0040\n",
      "Epoch 65/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2614 - acc: 0.9027 - val_loss: 12.8417 - val_acc: 0.0040\n",
      "Epoch 66/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2817 - acc: 0.8992 - val_loss: 12.2339 - val_acc: 0.0040\n",
      "Epoch 67/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2560 - acc: 0.9072 - val_loss: 12.8021 - val_acc: 0.0040\n",
      "Epoch 68/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2297 - acc: 0.9117 - val_loss: 12.1166 - val_acc: 0.0040\n",
      "Epoch 69/200\n",
      "4246/4246 [==============================] - 48s - loss: 0.2661 - acc: 0.9037 - val_loss: 13.2321 - val_acc: 0.0040\n",
      "Epoch 70/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.2584 - acc: 0.8997 - val_loss: 12.6750 - val_acc: 0.0040\n",
      "Epoch 71/200\n",
      "4246/4246 [==============================] - 56s - loss: 0.2512 - acc: 0.9117 - val_loss: 11.7750 - val_acc: 0.0040\n",
      "Epoch 72/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2394 - acc: 0.9093 - val_loss: 12.8502 - val_acc: 0.0040\n",
      "Epoch 73/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2410 - acc: 0.9107 - val_loss: 13.4109 - val_acc: 0.0040\n",
      "Epoch 74/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2356 - acc: 0.9166 - val_loss: 12.7059 - val_acc: 0.0040\n",
      "Epoch 75/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2314 - acc: 0.9140 - val_loss: 12.7776 - val_acc: 0.0040\n",
      "Epoch 76/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2242 - acc: 0.9152 - val_loss: 12.6299 - val_acc: 0.0040\n",
      "Epoch 77/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2215 - acc: 0.9145 - val_loss: 12.2041 - val_acc: 0.0040\n",
      "Epoch 78/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2196 - acc: 0.9164 - val_loss: 12.6971 - val_acc: 0.0040\n",
      "Epoch 79/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2353 - acc: 0.9154 - val_loss: 12.6411 - val_acc: 0.0040\n",
      "Epoch 80/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1930 - acc: 0.9249 - val_loss: 13.3623 - val_acc: 0.0040\n",
      "Epoch 81/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2107 - acc: 0.9251 - val_loss: 13.0792 - val_acc: 0.0040\n",
      "Epoch 82/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2129 - acc: 0.9218 - val_loss: 12.8987 - val_acc: 0.0040\n",
      "Epoch 83/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2308 - acc: 0.9187 - val_loss: 12.4309 - val_acc: 0.0040\n",
      "Epoch 84/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2027 - acc: 0.9260 - val_loss: 12.6275 - val_acc: 0.0040\n",
      "Epoch 85/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1864 - acc: 0.9277 - val_loss: 12.8949 - val_acc: 0.0040\n",
      "Epoch 86/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1895 - acc: 0.9253 - val_loss: 12.2721 - val_acc: 0.0040\n",
      "Epoch 87/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1840 - acc: 0.9315 - val_loss: 12.9904 - val_acc: 0.0040\n",
      "Epoch 88/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2029 - acc: 0.9270 - val_loss: 12.7734 - val_acc: 0.0040\n",
      "Epoch 89/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1979 - acc: 0.9301 - val_loss: 12.7993 - val_acc: 0.0040\n",
      "Epoch 90/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2235 - acc: 0.9162 - val_loss: 12.5941 - val_acc: 0.0040\n",
      "Epoch 91/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2290 - acc: 0.9145 - val_loss: 12.6945 - val_acc: 0.0040\n",
      "Epoch 92/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2283 - acc: 0.9133 - val_loss: 13.0672 - val_acc: 0.0040\n",
      "Epoch 93/200\n",
      "4246/4246 [==============================] - 52s - loss: 0.2113 - acc: 0.9235 - val_loss: 12.8667 - val_acc: 0.0040\n",
      "Epoch 94/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.1991 - acc: 0.9249 - val_loss: 12.8848 - val_acc: 0.0040\n",
      "Epoch 95/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.1932 - acc: 0.9293 - val_loss: 13.1671 - val_acc: 0.0040\n",
      "Epoch 96/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.2083 - acc: 0.9230 - val_loss: 12.6045 - val_acc: 0.0040\n",
      "Epoch 97/200\n",
      "4246/4246 [==============================] - 55s - loss: 0.1877 - acc: 0.9265 - val_loss: 13.4139 - val_acc: 0.0040\n",
      "Epoch 98/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1960 - acc: 0.9296 - val_loss: 13.2053 - val_acc: 0.0040\n",
      "Epoch 99/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2119 - acc: 0.9289 - val_loss: 12.9562 - val_acc: 0.0040\n",
      "Epoch 100/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2044 - acc: 0.9216 - val_loss: 12.9639 - val_acc: 0.0040\n",
      "Epoch 101/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2065 - acc: 0.9246 - val_loss: 13.5264 - val_acc: 0.0040\n",
      "Epoch 102/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1963 - acc: 0.9291 - val_loss: 12.7412 - val_acc: 0.0040\n",
      "Epoch 103/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1906 - acc: 0.9284 - val_loss: 13.1028 - val_acc: 0.0040\n",
      "Epoch 104/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1943 - acc: 0.9275 - val_loss: 12.9800 - val_acc: 0.0040\n",
      "Epoch 105/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2157 - acc: 0.9277 - val_loss: 12.8832 - val_acc: 0.0040\n",
      "Epoch 106/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1821 - acc: 0.9329 - val_loss: 13.4537 - val_acc: 0.0040\n",
      "Epoch 107/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1886 - acc: 0.9331 - val_loss: 13.8108 - val_acc: 0.0040\n",
      "Epoch 108/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1999 - acc: 0.9253 - val_loss: 13.1344 - val_acc: 0.0040\n",
      "Epoch 109/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1891 - acc: 0.9310 - val_loss: 13.1719 - val_acc: 0.0040\n",
      "Epoch 110/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1828 - acc: 0.9338 - val_loss: 13.4088 - val_acc: 0.0040\n",
      "Epoch 111/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1787 - acc: 0.9338 - val_loss: 13.7539 - val_acc: 0.0040\n",
      "Epoch 112/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2069 - acc: 0.9242 - val_loss: 13.5672 - val_acc: 0.0040\n",
      "Epoch 113/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2474 - acc: 0.9126 - val_loss: 13.7038 - val_acc: 0.0040\n",
      "Epoch 114/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2302 - acc: 0.9180 - val_loss: 13.1367 - val_acc: 0.0040\n",
      "Epoch 115/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2096 - acc: 0.9251 - val_loss: 14.0567 - val_acc: 0.0040\n",
      "Epoch 116/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2449 - acc: 0.9143 - val_loss: 13.5358 - val_acc: 0.0040\n",
      "Epoch 117/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2259 - acc: 0.9242 - val_loss: 12.8558 - val_acc: 0.0040\n",
      "Epoch 118/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2271 - acc: 0.9213 - val_loss: 13.6615 - val_acc: 0.0040\n",
      "Epoch 119/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2195 - acc: 0.9263 - val_loss: 13.8172 - val_acc: 0.0040\n",
      "Epoch 120/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2404 - acc: 0.9159 - val_loss: 13.4314 - val_acc: 0.0040\n",
      "Epoch 121/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2324 - acc: 0.9159 - val_loss: 12.3543 - val_acc: 0.0040\n",
      "Epoch 122/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2125 - acc: 0.9251 - val_loss: 13.6833 - val_acc: 0.0040\n",
      "Epoch 123/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2128 - acc: 0.9277 - val_loss: 12.7799 - val_acc: 0.0040\n",
      "Epoch 124/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1805 - acc: 0.9359 - val_loss: 13.4503 - val_acc: 0.0040\n",
      "Epoch 125/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1616 - acc: 0.9409 - val_loss: 13.7282 - val_acc: 0.0040\n",
      "Epoch 126/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1918 - acc: 0.9305 - val_loss: 13.4122 - val_acc: 0.0040\n",
      "Epoch 127/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2087 - acc: 0.9279 - val_loss: 13.4782 - val_acc: 0.0040\n",
      "Epoch 128/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1942 - acc: 0.9286 - val_loss: 13.4460 - val_acc: 0.0040\n",
      "Epoch 129/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1734 - acc: 0.9376 - val_loss: 13.0497 - val_acc: 0.0040\n",
      "Epoch 130/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1704 - acc: 0.9333 - val_loss: 13.2315 - val_acc: 0.0040\n",
      "Epoch 131/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1680 - acc: 0.9423 - val_loss: 13.5823 - val_acc: 0.0040\n",
      "Epoch 132/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1830 - acc: 0.9397 - val_loss: 13.2156 - val_acc: 0.0040\n",
      "Epoch 133/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1634 - acc: 0.9388 - val_loss: 13.6106 - val_acc: 0.0040\n",
      "Epoch 134/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1785 - acc: 0.9341 - val_loss: 13.2226 - val_acc: 0.0040\n",
      "Epoch 135/200\n",
      "4246/4246 [==============================] - 53s - loss: 0.1987 - acc: 0.9296 - val_loss: 13.3125 - val_acc: 0.0040\n",
      "Epoch 136/200\n",
      "4246/4246 [==============================] - 48s - loss: 0.2048 - acc: 0.9284 - val_loss: 13.7306 - val_acc: 0.0040\n",
      "Epoch 137/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.2075 - acc: 0.9291 - val_loss: 13.5517 - val_acc: 0.0040\n",
      "Epoch 138/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2140 - acc: 0.9251 - val_loss: 13.2981 - val_acc: 0.0040\n",
      "Epoch 139/200\n",
      "4246/4246 [==============================] - 55s - loss: 0.1893 - acc: 0.9317 - val_loss: 13.8361 - val_acc: 0.0040\n",
      "Epoch 140/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.1875 - acc: 0.9312 - val_loss: 13.9544 - val_acc: 0.0040\n",
      "Epoch 141/200\n",
      "4246/4246 [==============================] - 47s - loss: 0.2362 - acc: 0.9199 - val_loss: 13.9678 - val_acc: 0.0040\n",
      "Epoch 142/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2675 - acc: 0.9143 - val_loss: 13.1743 - val_acc: 0.0040\n",
      "Epoch 143/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2488 - acc: 0.9176 - val_loss: 13.6088 - val_acc: 0.0040\n",
      "Epoch 144/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2318 - acc: 0.9242 - val_loss: 12.9521 - val_acc: 0.0040\n",
      "Epoch 145/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1978 - acc: 0.9272 - val_loss: 13.4642 - val_acc: 0.0040\n",
      "Epoch 146/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1893 - acc: 0.9310 - val_loss: 13.9217 - val_acc: 0.0040\n",
      "Epoch 147/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2072 - acc: 0.9268 - val_loss: 13.8722 - val_acc: 0.0040\n",
      "Epoch 148/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1995 - acc: 0.9303 - val_loss: 13.4619 - val_acc: 0.0040\n",
      "Epoch 149/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2216 - acc: 0.9251 - val_loss: 13.1532 - val_acc: 0.0040\n",
      "Epoch 150/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1922 - acc: 0.9343 - val_loss: 13.2316 - val_acc: 0.0040\n",
      "Epoch 151/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2096 - acc: 0.9277 - val_loss: 12.8857 - val_acc: 0.0040\n",
      "Epoch 152/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1989 - acc: 0.9317 - val_loss: 12.9082 - val_acc: 0.0040\n",
      "Epoch 153/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2447 - acc: 0.9152 - val_loss: 13.5006 - val_acc: 0.0040\n",
      "Epoch 154/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2246 - acc: 0.9171 - val_loss: 13.6756 - val_acc: 0.0040\n",
      "Epoch 155/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2474 - acc: 0.9204 - val_loss: 13.3934 - val_acc: 0.0040\n",
      "Epoch 156/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2221 - acc: 0.9244 - val_loss: 13.4131 - val_acc: 0.0040\n",
      "Epoch 157/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2270 - acc: 0.9246 - val_loss: 13.0772 - val_acc: 0.0040\n",
      "Epoch 158/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2466 - acc: 0.9154 - val_loss: 13.8114 - val_acc: 0.0040\n",
      "Epoch 159/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1993 - acc: 0.9308 - val_loss: 13.4716 - val_acc: 0.0040\n",
      "Epoch 160/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2239 - acc: 0.9242 - val_loss: 13.4147 - val_acc: 0.0040\n",
      "Epoch 161/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1706 - acc: 0.9430 - val_loss: 13.5652 - val_acc: 0.0040\n",
      "Epoch 162/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1841 - acc: 0.9352 - val_loss: 13.3766 - val_acc: 0.0040\n",
      "Epoch 163/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1955 - acc: 0.9345 - val_loss: 13.4238 - val_acc: 0.0040\n",
      "Epoch 164/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2446 - acc: 0.9176 - val_loss: 13.0276 - val_acc: 0.0040\n",
      "Epoch 165/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.2388 - acc: 0.9187 - val_loss: 13.2985 - val_acc: 0.0040\n",
      "Epoch 166/200\n",
      "4246/4246 [==============================] - 53s - loss: 0.1864 - acc: 0.9308 - val_loss: 12.9563 - val_acc: 0.0040\n",
      "Epoch 167/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.1662 - acc: 0.9402 - val_loss: 13.7830 - val_acc: 0.0040\n",
      "Epoch 168/200\n",
      "4246/4246 [==============================] - 59s - loss: 0.1799 - acc: 0.9397 - val_loss: 12.9115 - val_acc: 0.0040\n",
      "Epoch 169/200\n",
      "4246/4246 [==============================] - 52s - loss: 0.2017 - acc: 0.9296 - val_loss: 13.7486 - val_acc: 0.0040\n",
      "Epoch 170/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1788 - acc: 0.9381 - val_loss: 13.3689 - val_acc: 0.0040\n",
      "Epoch 171/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1694 - acc: 0.9366 - val_loss: 13.7174 - val_acc: 0.0040\n",
      "Epoch 172/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1924 - acc: 0.9402 - val_loss: 12.7722 - val_acc: 0.0040\n",
      "Epoch 173/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1708 - acc: 0.9390 - val_loss: 12.8606 - val_acc: 0.0040\n",
      "Epoch 174/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1736 - acc: 0.9418 - val_loss: 13.0740 - val_acc: 0.0040\n",
      "Epoch 175/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1681 - acc: 0.9388 - val_loss: 13.3618 - val_acc: 0.0040\n",
      "Epoch 176/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1682 - acc: 0.9371 - val_loss: 13.4039 - val_acc: 0.0040\n",
      "Epoch 177/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1404 - acc: 0.9491 - val_loss: 13.0110 - val_acc: 0.0040\n",
      "Epoch 178/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1380 - acc: 0.9480 - val_loss: 12.9493 - val_acc: 0.0040\n",
      "Epoch 179/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1504 - acc: 0.9432 - val_loss: 13.3781 - val_acc: 0.0040\n",
      "Epoch 180/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.1561 - acc: 0.9454 - val_loss: 13.6741 - val_acc: 0.0040\n",
      "Epoch 181/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1399 - acc: 0.9496 - val_loss: 13.7156 - val_acc: 0.0040\n",
      "Epoch 182/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1511 - acc: 0.9468 - val_loss: 13.0304 - val_acc: 0.0040\n",
      "Epoch 183/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1478 - acc: 0.9489 - val_loss: 13.0746 - val_acc: 0.0040\n",
      "Epoch 184/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1465 - acc: 0.9494 - val_loss: 13.5700 - val_acc: 0.0040\n",
      "Epoch 185/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1540 - acc: 0.9432 - val_loss: 13.3978 - val_acc: 0.0040\n",
      "Epoch 186/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1496 - acc: 0.9489 - val_loss: 13.3530 - val_acc: 0.0040\n",
      "Epoch 187/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1850 - acc: 0.9369 - val_loss: 13.2403 - val_acc: 0.0040\n",
      "Epoch 188/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.2001 - acc: 0.9348 - val_loss: 13.9368 - val_acc: 0.0040\n",
      "Epoch 189/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1895 - acc: 0.9343 - val_loss: 12.7272 - val_acc: 0.0040\n",
      "Epoch 190/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1627 - acc: 0.9442 - val_loss: 13.4972 - val_acc: 0.0040\n",
      "Epoch 191/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1661 - acc: 0.9430 - val_loss: 13.2900 - val_acc: 0.0040\n",
      "Epoch 192/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1734 - acc: 0.9378 - val_loss: 13.2184 - val_acc: 0.0040\n",
      "Epoch 193/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1786 - acc: 0.9390 - val_loss: 13.5502 - val_acc: 0.0040\n",
      "Epoch 194/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1661 - acc: 0.9418 - val_loss: 14.0641 - val_acc: 0.0040\n",
      "Epoch 195/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1730 - acc: 0.9437 - val_loss: 13.1978 - val_acc: 0.0040\n",
      "Epoch 196/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1646 - acc: 0.9390 - val_loss: 13.6658 - val_acc: 0.0040\n",
      "Epoch 197/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1641 - acc: 0.9435 - val_loss: 13.3325 - val_acc: 0.0040\n",
      "Epoch 198/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1547 - acc: 0.9465 - val_loss: 13.2462 - val_acc: 0.0040\n",
      "Epoch 199/200\n",
      "4246/4246 [==============================] - 45s - loss: 0.1362 - acc: 0.9496 - val_loss: 13.9918 - val_acc: 0.0040\n",
      "Epoch 200/200\n",
      "4246/4246 [==============================] - 44s - loss: 0.1337 - acc: 0.9510 - val_loss: 13.6339 - val_acc: 0.0040\n",
      "Test loss: 2.96140685794\n",
      "Test accuracy: 0.700100300903\n"
     ]
    }
   ],
   "source": [
    "# The actual training of the CNN using the parameters and model previously specified.\n",
    "# The validation set is a split of the stratified sampled training data.\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split = 0.15)\n",
    "\n",
    "# Evaluate the performance on the unused testing set.\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('tuned_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x1154fd210>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1154fd290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x103faa490>,\n",
       " <keras.layers.core.Dropout at 0x1154fd8d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1154fd9d0>,\n",
       " <keras.layers.core.Flatten at 0x10402b710>,\n",
       " <keras.layers.core.Dense at 0x103fc6b90>,\n",
       " <keras.layers.core.Dropout at 0x103fc6f90>,\n",
       " <keras.layers.core.Dense at 0x104056bd0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAFtCAYAAAAaiCMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1193aadd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here is a visualization of the training process\n",
    "# typically we gain a lot in the beginning and then\n",
    "# training slows down\n",
    "plt.plot(history.history['acc'])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Training Accuracy, Baseline Model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that this model with 200 epochs (which includes some slight tuning) yielded an impressive accuracy of 70% on the unseen test set, and 95% accuracy on the training set.  Next, we will add another convultional, dropout, and pooling layer and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_97 (Conv2D)           (None, 134, 88, 16)       1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 44, 29, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 42, 27, 32)        4640      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 42, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 14, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_99 (Conv2D)           (None, 12, 7, 48)         13872     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12, 7, 48)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 2, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                24640     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 44,823\n",
      "Trainable params: 44,823\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an empty network model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model2.add(Conv2D(16, kernel_size=input_kernel_size, activation=input_activation_function, input_shape=input_shape))\n",
    "model2.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Hidden Layer(s)\n",
    "model2.add(Conv2D(32, kernel_size=hidden_kernel_size, activation=hidden_activation_function))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model2.add(Conv2D(48, kernel_size=hidden_kernel_size, activation=hidden_activation_function))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Classification layer\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(64, activation=hidden_activation_function))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(classes, activation=final_activation_function))\n",
    "\n",
    "# Display the CNN.\n",
    "model2.summary()\n",
    "\n",
    "# Compile the model.\n",
    "model2.compile(loss=loss_method, optimizer=optimizer, metrics=[eval_metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4246 samples, validate on 750 samples\n",
      "Epoch 1/200\n",
      "4246/4246 [==============================] - 48s - loss: 1.6543 - acc: 0.2812 - val_loss: 4.9307 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "4246/4246 [==============================] - 47s - loss: 1.5931 - acc: 0.3069 - val_loss: 4.0383 - val_acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.5346 - acc: 0.3420 - val_loss: 4.1623 - val_acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.4778 - acc: 0.3846 - val_loss: 4.7397 - val_acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.4699 - acc: 0.3815 - val_loss: 5.1063 - val_acc: 0.0000e+00\n",
      "Epoch 6/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.4468 - acc: 0.4103 - val_loss: 5.3261 - val_acc: 0.0000e+00\n",
      "Epoch 7/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.4355 - acc: 0.4178 - val_loss: 4.9961 - val_acc: 0.0000e+00\n",
      "Epoch 8/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.4327 - acc: 0.4025 - val_loss: 5.0772 - val_acc: 0.0000e+00\n",
      "Epoch 9/200\n",
      "4246/4246 [==============================] - 47s - loss: 1.4198 - acc: 0.4159 - val_loss: 5.5057 - val_acc: 0.0000e+00\n",
      "Epoch 10/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.4146 - acc: 0.4216 - val_loss: 5.4646 - val_acc: 0.0000e+00\n",
      "Epoch 11/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.3995 - acc: 0.4303 - val_loss: 5.6929 - val_acc: 0.0000e+00\n",
      "Epoch 12/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.3927 - acc: 0.4322 - val_loss: 5.7267 - val_acc: 0.0000e+00\n",
      "Epoch 13/200\n",
      "4246/4246 [==============================] - 47s - loss: 1.3677 - acc: 0.4456 - val_loss: 5.7643 - val_acc: 0.0000e+00\n",
      "Epoch 14/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.3564 - acc: 0.4447 - val_loss: 5.8028 - val_acc: 0.0000e+00\n",
      "Epoch 15/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.3480 - acc: 0.4600 - val_loss: 6.2505 - val_acc: 0.0000e+00\n",
      "Epoch 16/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.3323 - acc: 0.4520 - val_loss: 5.8724 - val_acc: 0.0000e+00\n",
      "Epoch 17/200\n",
      "4246/4246 [==============================] - 48s - loss: 1.3165 - acc: 0.4635 - val_loss: 6.4591 - val_acc: 0.0000e+00\n",
      "Epoch 18/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.3063 - acc: 0.4597 - val_loss: 5.7935 - val_acc: 0.0000e+00\n",
      "Epoch 19/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.2893 - acc: 0.4708 - val_loss: 6.2174 - val_acc: 0.0000e+00\n",
      "Epoch 20/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.2633 - acc: 0.4863 - val_loss: 6.0547 - val_acc: 0.0000e+00\n",
      "Epoch 21/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.2402 - acc: 0.4981 - val_loss: 6.0126 - val_acc: 0.0000e+00\n",
      "Epoch 22/200\n",
      "4246/4246 [==============================] - 47s - loss: 1.2331 - acc: 0.4925 - val_loss: 6.9671 - val_acc: 0.0000e+00\n",
      "Epoch 23/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.1931 - acc: 0.5179 - val_loss: 6.8554 - val_acc: 0.0000e+00\n",
      "Epoch 24/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.1637 - acc: 0.5386 - val_loss: 6.5733 - val_acc: 0.0000e+00\n",
      "Epoch 25/200\n",
      "4246/4246 [==============================] - 47s - loss: 1.1458 - acc: 0.5405 - val_loss: 6.8441 - val_acc: 0.0000e+00\n",
      "Epoch 26/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.1119 - acc: 0.5608 - val_loss: 7.2572 - val_acc: 0.0000e+00\n",
      "Epoch 27/200\n",
      "4246/4246 [==============================] - 47s - loss: 1.1178 - acc: 0.5617 - val_loss: 6.6655 - val_acc: 0.0000e+00\n",
      "Epoch 28/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.1141 - acc: 0.5561 - val_loss: 6.9619 - val_acc: 0.0000e+00\n",
      "Epoch 29/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.0671 - acc: 0.5730 - val_loss: 7.5127 - val_acc: 0.0000e+00\n",
      "Epoch 30/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.0853 - acc: 0.5742 - val_loss: 7.4175 - val_acc: 0.0000e+00\n",
      "Epoch 31/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.0683 - acc: 0.5780 - val_loss: 7.2906 - val_acc: 0.0000e+00\n",
      "Epoch 32/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.0089 - acc: 0.6090 - val_loss: 7.8413 - val_acc: 0.0000e+00\n",
      "Epoch 33/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.0403 - acc: 0.5959 - val_loss: 6.8133 - val_acc: 0.0000e+00\n",
      "Epoch 34/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.0594 - acc: 0.5869 - val_loss: 7.6736 - val_acc: 0.0000e+00\n",
      "Epoch 35/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.9954 - acc: 0.6034 - val_loss: 7.7872 - val_acc: 0.0000e+00\n",
      "Epoch 36/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.0262 - acc: 0.5926 - val_loss: 6.7215 - val_acc: 0.0000e+00\n",
      "Epoch 37/200\n",
      "4246/4246 [==============================] - 46s - loss: 1.0837 - acc: 0.5676 - val_loss: 7.3936 - val_acc: 0.0013\n",
      "Epoch 38/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.9984 - acc: 0.6182 - val_loss: 8.0817 - val_acc: 0.0000e+00\n",
      "Epoch 39/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.9283 - acc: 0.6465 - val_loss: 8.0904 - val_acc: 0.0013\n",
      "Epoch 40/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.9106 - acc: 0.6510 - val_loss: 8.2826 - val_acc: 0.0000e+00\n",
      "Epoch 41/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.9228 - acc: 0.6488 - val_loss: 7.7098 - val_acc: 0.0000e+00\n",
      "Epoch 42/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.8933 - acc: 0.6455 - val_loss: 8.4293 - val_acc: 0.0000e+00\n",
      "Epoch 43/200\n",
      "4246/4246 [==============================] - 46s - loss: 0.8677 - acc: 0.6597 - val_loss: 8.4770 - val_acc: 0.0000e+00\n",
      "Epoch 44/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.9040 - acc: 0.6474 - val_loss: 8.8484 - val_acc: 0.0013\n",
      "Epoch 45/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.9498 - acc: 0.6390 - val_loss: 7.8537 - val_acc: 0.0013\n",
      "Epoch 46/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.9281 - acc: 0.6361 - val_loss: 8.0037 - val_acc: 0.0013\n",
      "Epoch 47/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.9205 - acc: 0.6425 - val_loss: 7.6085 - val_acc: 0.0000e+00\n",
      "Epoch 48/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.8929 - acc: 0.6564 - val_loss: 8.6159 - val_acc: 0.0013\n",
      "Epoch 49/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.8925 - acc: 0.6613 - val_loss: 8.3348 - val_acc: 0.0013\n",
      "Epoch 50/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.8819 - acc: 0.6656 - val_loss: 8.6284 - val_acc: 0.0013\n",
      "Epoch 51/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.8803 - acc: 0.6547 - val_loss: 8.2310 - val_acc: 0.0013\n",
      "Epoch 52/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.8504 - acc: 0.6710 - val_loss: 8.3014 - val_acc: 0.0027\n",
      "Epoch 53/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.8051 - acc: 0.6889 - val_loss: 8.5665 - val_acc: 0.0013\n",
      "Epoch 54/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7990 - acc: 0.6938 - val_loss: 9.8165 - val_acc: 0.0040\n",
      "Epoch 55/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7692 - acc: 0.7011 - val_loss: 8.9086 - val_acc: 0.0013\n",
      "Epoch 56/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.8155 - acc: 0.6823 - val_loss: 9.1736 - val_acc: 0.0013\n",
      "Epoch 57/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7660 - acc: 0.7063 - val_loss: 9.2200 - val_acc: 0.0000e+00\n",
      "Epoch 58/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7779 - acc: 0.7004 - val_loss: 8.8094 - val_acc: 0.0027\n",
      "Epoch 59/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.8316 - acc: 0.6766 - val_loss: 8.8835 - val_acc: 0.0040\n",
      "Epoch 60/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7973 - acc: 0.6978 - val_loss: 8.8544 - val_acc: 0.0013\n",
      "Epoch 61/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7349 - acc: 0.7228 - val_loss: 9.8787 - val_acc: 0.0013\n",
      "Epoch 62/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7714 - acc: 0.7084 - val_loss: 9.0893 - val_acc: 0.0040\n",
      "Epoch 63/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7780 - acc: 0.7016 - val_loss: 8.8863 - val_acc: 0.0027\n",
      "Epoch 64/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7666 - acc: 0.7077 - val_loss: 9.4003 - val_acc: 0.0027\n",
      "Epoch 65/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7426 - acc: 0.7209 - val_loss: 9.2093 - val_acc: 0.0027\n",
      "Epoch 66/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7629 - acc: 0.7221 - val_loss: 9.4273 - val_acc: 0.0040\n",
      "Epoch 67/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7597 - acc: 0.7117 - val_loss: 9.0988 - val_acc: 0.0027\n",
      "Epoch 68/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7721 - acc: 0.7049 - val_loss: 9.7775 - val_acc: 0.0027\n",
      "Epoch 69/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7453 - acc: 0.7256 - val_loss: 9.3751 - val_acc: 0.0000e+00\n",
      "Epoch 70/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7193 - acc: 0.7277 - val_loss: 9.5695 - val_acc: 0.0027\n",
      "Epoch 71/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7259 - acc: 0.7336 - val_loss: 8.7061 - val_acc: 0.0027\n",
      "Epoch 72/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7362 - acc: 0.7247 - val_loss: 9.4556 - val_acc: 0.0013\n",
      "Epoch 73/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7484 - acc: 0.7197 - val_loss: 9.3207 - val_acc: 0.0027\n",
      "Epoch 74/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7374 - acc: 0.7282 - val_loss: 8.7937 - val_acc: 0.0040\n",
      "Epoch 75/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6961 - acc: 0.7390 - val_loss: 9.9611 - val_acc: 0.0040\n",
      "Epoch 76/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7223 - acc: 0.7383 - val_loss: 8.9416 - val_acc: 0.0040\n",
      "Epoch 77/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6682 - acc: 0.7438 - val_loss: 10.0017 - val_acc: 0.0027\n",
      "Epoch 78/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7347 - acc: 0.7301 - val_loss: 9.3874 - val_acc: 0.0013\n",
      "Epoch 79/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7048 - acc: 0.7402 - val_loss: 9.3161 - val_acc: 0.0040\n",
      "Epoch 80/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7226 - acc: 0.7332 - val_loss: 9.5186 - val_acc: 0.0027\n",
      "Epoch 81/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7347 - acc: 0.7252 - val_loss: 8.8455 - val_acc: 0.0027\n",
      "Epoch 82/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.6872 - acc: 0.7518 - val_loss: 9.7030 - val_acc: 0.0040\n",
      "Epoch 83/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6797 - acc: 0.7494 - val_loss: 9.8108 - val_acc: 0.0040\n",
      "Epoch 84/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7346 - acc: 0.7350 - val_loss: 9.5612 - val_acc: 0.0013\n",
      "Epoch 85/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6986 - acc: 0.7480 - val_loss: 10.1559 - val_acc: 0.0040\n",
      "Epoch 86/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7164 - acc: 0.7275 - val_loss: 10.8393 - val_acc: 0.0027\n",
      "Epoch 87/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.8142 - acc: 0.7054 - val_loss: 8.9857 - val_acc: 0.0013\n",
      "Epoch 88/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7522 - acc: 0.7207 - val_loss: 9.4241 - val_acc: 0.0027\n",
      "Epoch 89/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6946 - acc: 0.7478 - val_loss: 9.9598 - val_acc: 0.0027\n",
      "Epoch 90/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7474 - acc: 0.7277 - val_loss: 9.1303 - val_acc: 0.0027\n",
      "Epoch 91/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6926 - acc: 0.7459 - val_loss: 10.0595 - val_acc: 0.0013\n",
      "Epoch 92/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6567 - acc: 0.7518 - val_loss: 10.1695 - val_acc: 0.0027\n",
      "Epoch 93/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7149 - acc: 0.7459 - val_loss: 9.5350 - val_acc: 0.0013\n",
      "Epoch 94/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7688 - acc: 0.7214 - val_loss: 10.1851 - val_acc: 0.0013\n",
      "Epoch 95/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7375 - acc: 0.7341 - val_loss: 9.2299 - val_acc: 0.0027\n",
      "Epoch 96/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6752 - acc: 0.7562 - val_loss: 9.9369 - val_acc: 0.0027\n",
      "Epoch 97/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7321 - acc: 0.7327 - val_loss: 9.1037 - val_acc: 0.0027\n",
      "Epoch 98/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7398 - acc: 0.7273 - val_loss: 9.4007 - val_acc: 0.0040\n",
      "Epoch 99/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7755 - acc: 0.7230 - val_loss: 9.3335 - val_acc: 0.0013\n",
      "Epoch 100/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7713 - acc: 0.7221 - val_loss: 9.6772 - val_acc: 0.0013\n",
      "Epoch 101/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7224 - acc: 0.7381 - val_loss: 9.5004 - val_acc: 0.0027\n",
      "Epoch 102/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7219 - acc: 0.7447 - val_loss: 10.4041 - val_acc: 0.0027\n",
      "Epoch 103/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7282 - acc: 0.7360 - val_loss: 10.6462 - val_acc: 0.0013\n",
      "Epoch 104/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6759 - acc: 0.7626 - val_loss: 9.7886 - val_acc: 0.0027\n",
      "Epoch 105/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6639 - acc: 0.7619 - val_loss: 9.3040 - val_acc: 0.0027\n",
      "Epoch 106/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6554 - acc: 0.7678 - val_loss: 9.7366 - val_acc: 0.0040\n",
      "Epoch 107/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.6916 - acc: 0.7463 - val_loss: 10.3682 - val_acc: 0.0027\n",
      "Epoch 108/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7280 - acc: 0.7379 - val_loss: 8.9219 - val_acc: 0.0027\n",
      "Epoch 109/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7094 - acc: 0.7402 - val_loss: 9.3900 - val_acc: 0.0000e+00\n",
      "Epoch 110/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7648 - acc: 0.7301 - val_loss: 10.0271 - val_acc: 0.0013\n",
      "Epoch 111/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.6900 - acc: 0.7478 - val_loss: 10.1493 - val_acc: 0.0027\n",
      "Epoch 112/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6908 - acc: 0.7475 - val_loss: 9.6455 - val_acc: 0.0040\n",
      "Epoch 113/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6595 - acc: 0.7607 - val_loss: 9.6973 - val_acc: 0.0040\n",
      "Epoch 114/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7292 - acc: 0.7456 - val_loss: 9.5485 - val_acc: 0.0027\n",
      "Epoch 115/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6611 - acc: 0.7624 - val_loss: 10.1871 - val_acc: 0.0027\n",
      "Epoch 116/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6886 - acc: 0.7537 - val_loss: 10.0778 - val_acc: 0.0027\n",
      "Epoch 117/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6975 - acc: 0.7471 - val_loss: 10.0438 - val_acc: 0.0027\n",
      "Epoch 118/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6617 - acc: 0.7584 - val_loss: 10.4678 - val_acc: 0.0027\n",
      "Epoch 119/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6338 - acc: 0.7711 - val_loss: 10.7867 - val_acc: 0.0013\n",
      "Epoch 120/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7465 - acc: 0.7332 - val_loss: 10.9384 - val_acc: 0.0040\n",
      "Epoch 121/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6804 - acc: 0.7565 - val_loss: 9.7673 - val_acc: 0.0040\n",
      "Epoch 122/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6941 - acc: 0.7555 - val_loss: 10.2308 - val_acc: 0.0027\n",
      "Epoch 123/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6714 - acc: 0.7657 - val_loss: 10.2469 - val_acc: 0.0040\n",
      "Epoch 124/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6912 - acc: 0.7520 - val_loss: 9.5807 - val_acc: 0.0040\n",
      "Epoch 125/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6917 - acc: 0.7492 - val_loss: 10.7000 - val_acc: 0.0040\n",
      "Epoch 126/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7092 - acc: 0.7463 - val_loss: 10.9108 - val_acc: 0.0013\n",
      "Epoch 127/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6630 - acc: 0.7704 - val_loss: 10.7820 - val_acc: 0.0013\n",
      "Epoch 128/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.6729 - acc: 0.7612 - val_loss: 10.1051 - val_acc: 0.0027\n",
      "Epoch 129/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6483 - acc: 0.7678 - val_loss: 9.3876 - val_acc: 0.0027\n",
      "Epoch 130/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6644 - acc: 0.7647 - val_loss: 10.5910 - val_acc: 0.0027\n",
      "Epoch 131/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7317 - acc: 0.7475 - val_loss: 10.2135 - val_acc: 0.0040\n",
      "Epoch 132/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7630 - acc: 0.7376 - val_loss: 8.9396 - val_acc: 0.0040\n",
      "Epoch 133/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7591 - acc: 0.7341 - val_loss: 10.2474 - val_acc: 0.0040\n",
      "Epoch 134/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7472 - acc: 0.7412 - val_loss: 9.3397 - val_acc: 0.0040\n",
      "Epoch 135/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6937 - acc: 0.7435 - val_loss: 10.4522 - val_acc: 0.0027\n",
      "Epoch 136/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6194 - acc: 0.7869 - val_loss: 10.0054 - val_acc: 0.0027\n",
      "Epoch 137/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7778 - acc: 0.7339 - val_loss: 9.5037 - val_acc: 0.0040\n",
      "Epoch 138/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6811 - acc: 0.7673 - val_loss: 10.5293 - val_acc: 0.0027\n",
      "Epoch 139/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6514 - acc: 0.7680 - val_loss: 10.2180 - val_acc: 0.0040\n",
      "Epoch 140/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6792 - acc: 0.7631 - val_loss: 10.1332 - val_acc: 0.0040\n",
      "Epoch 141/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7089 - acc: 0.7541 - val_loss: 10.2003 - val_acc: 0.0013\n",
      "Epoch 142/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7167 - acc: 0.7534 - val_loss: 9.7133 - val_acc: 0.0013\n",
      "Epoch 143/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7104 - acc: 0.7499 - val_loss: 10.0663 - val_acc: 0.0013\n",
      "Epoch 144/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6703 - acc: 0.7633 - val_loss: 10.2451 - val_acc: 0.0013\n",
      "Epoch 145/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7003 - acc: 0.7506 - val_loss: 9.8081 - val_acc: 0.0040\n",
      "Epoch 146/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6585 - acc: 0.7605 - val_loss: 10.8591 - val_acc: 0.0027\n",
      "Epoch 147/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7162 - acc: 0.7541 - val_loss: 9.7047 - val_acc: 0.0040\n",
      "Epoch 148/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7540 - acc: 0.7468 - val_loss: 9.8108 - val_acc: 0.0027\n",
      "Epoch 149/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7285 - acc: 0.7489 - val_loss: 9.9970 - val_acc: 0.0040\n",
      "Epoch 150/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6417 - acc: 0.7767 - val_loss: 10.7716 - val_acc: 0.0040\n",
      "Epoch 151/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7127 - acc: 0.7489 - val_loss: 10.9927 - val_acc: 0.0040\n",
      "Epoch 152/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6938 - acc: 0.7562 - val_loss: 10.5830 - val_acc: 0.0027\n",
      "Epoch 153/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6829 - acc: 0.7687 - val_loss: 10.0827 - val_acc: 0.0040\n",
      "Epoch 154/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6117 - acc: 0.7833 - val_loss: 10.4504 - val_acc: 0.0027\n",
      "Epoch 155/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6565 - acc: 0.7711 - val_loss: 10.8042 - val_acc: 0.0013\n",
      "Epoch 156/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6449 - acc: 0.7699 - val_loss: 10.3649 - val_acc: 0.0027\n",
      "Epoch 157/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6737 - acc: 0.7793 - val_loss: 9.4061 - val_acc: 0.0027\n",
      "Epoch 158/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6667 - acc: 0.7737 - val_loss: 9.7507 - val_acc: 0.0027\n",
      "Epoch 159/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6324 - acc: 0.7918 - val_loss: 10.0627 - val_acc: 0.0040\n",
      "Epoch 160/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6199 - acc: 0.7831 - val_loss: 10.7593 - val_acc: 0.0027\n",
      "Epoch 161/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6589 - acc: 0.7748 - val_loss: 10.0019 - val_acc: 0.0040\n",
      "Epoch 162/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7211 - acc: 0.7572 - val_loss: 9.4587 - val_acc: 0.0013\n",
      "Epoch 163/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6384 - acc: 0.7751 - val_loss: 10.1550 - val_acc: 0.0027\n",
      "Epoch 164/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6663 - acc: 0.7753 - val_loss: 10.5507 - val_acc: 0.0013\n",
      "Epoch 165/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6059 - acc: 0.7873 - val_loss: 10.8503 - val_acc: 0.0013\n",
      "Epoch 166/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6581 - acc: 0.7819 - val_loss: 10.5434 - val_acc: 0.0013\n",
      "Epoch 167/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.6600 - acc: 0.7744 - val_loss: 9.9095 - val_acc: 0.0027\n",
      "Epoch 168/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6169 - acc: 0.7850 - val_loss: 10.0534 - val_acc: 0.0027\n",
      "Epoch 169/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7527 - acc: 0.7475 - val_loss: 10.8539 - val_acc: 0.0013\n",
      "Epoch 170/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7137 - acc: 0.7572 - val_loss: 10.4685 - val_acc: 0.0027\n",
      "Epoch 171/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6896 - acc: 0.7694 - val_loss: 10.2484 - val_acc: 0.0027\n",
      "Epoch 172/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7105 - acc: 0.7588 - val_loss: 11.1754 - val_acc: 0.0027\n",
      "Epoch 173/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6613 - acc: 0.7760 - val_loss: 10.4675 - val_acc: 0.0027\n",
      "Epoch 174/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6023 - acc: 0.7960 - val_loss: 10.6687 - val_acc: 0.0027\n",
      "Epoch 175/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7979 - acc: 0.7221 - val_loss: 9.9524 - val_acc: 0.0027\n",
      "Epoch 176/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7394 - acc: 0.7456 - val_loss: 10.3615 - val_acc: 0.0027\n",
      "Epoch 177/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6056 - acc: 0.7930 - val_loss: 10.3804 - val_acc: 0.0040\n",
      "Epoch 178/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6526 - acc: 0.7852 - val_loss: 11.2362 - val_acc: 0.0027\n",
      "Epoch 179/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7016 - acc: 0.7607 - val_loss: 10.8682 - val_acc: 0.0040\n",
      "Epoch 180/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6293 - acc: 0.7902 - val_loss: 10.2922 - val_acc: 0.0040\n",
      "Epoch 181/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6551 - acc: 0.7730 - val_loss: 9.8685 - val_acc: 0.0027\n",
      "Epoch 182/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7135 - acc: 0.7541 - val_loss: 11.0323 - val_acc: 0.0013\n",
      "Epoch 183/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.6683 - acc: 0.7715 - val_loss: 10.3755 - val_acc: 0.0013\n",
      "Epoch 184/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6837 - acc: 0.7584 - val_loss: 10.6354 - val_acc: 0.0013\n",
      "Epoch 185/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6442 - acc: 0.7781 - val_loss: 10.9696 - val_acc: 0.0013\n",
      "Epoch 186/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6572 - acc: 0.7800 - val_loss: 9.6747 - val_acc: 0.0027\n",
      "Epoch 187/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6490 - acc: 0.7774 - val_loss: 10.2108 - val_acc: 0.0013\n",
      "Epoch 188/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.6486 - acc: 0.7760 - val_loss: 10.6893 - val_acc: 0.0013\n",
      "Epoch 189/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6646 - acc: 0.7779 - val_loss: 10.3284 - val_acc: 0.0027\n",
      "Epoch 190/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6628 - acc: 0.7774 - val_loss: 10.1757 - val_acc: 0.0013\n",
      "Epoch 191/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7455 - acc: 0.7489 - val_loss: 10.2112 - val_acc: 0.0013\n",
      "Epoch 192/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6507 - acc: 0.7741 - val_loss: 10.9271 - val_acc: 0.0013\n",
      "Epoch 193/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6478 - acc: 0.7803 - val_loss: 9.9182 - val_acc: 0.0000e+00\n",
      "Epoch 194/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6580 - acc: 0.7765 - val_loss: 10.5871 - val_acc: 0.0027\n",
      "Epoch 195/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.6759 - acc: 0.7838 - val_loss: 11.4325 - val_acc: 0.0013\n",
      "Epoch 196/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.6531 - acc: 0.7862 - val_loss: 10.5200 - val_acc: 0.0027\n",
      "Epoch 197/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7272 - acc: 0.7586 - val_loss: 10.3617 - val_acc: 0.0027\n",
      "Epoch 198/200\n",
      "4246/4246 [==============================] - 61s - loss: 0.7795 - acc: 0.7327 - val_loss: 9.5595 - val_acc: 0.0027\n",
      "Epoch 199/200\n",
      "4246/4246 [==============================] - 62s - loss: 0.7362 - acc: 0.7518 - val_loss: 10.3716 - val_acc: 0.0013\n",
      "Epoch 200/200\n",
      "4246/4246 [==============================] - 60s - loss: 0.7326 - acc: 0.7402 - val_loss: 10.6330 - val_acc: 0.0000e+00\n",
      "Test loss: 2.31069281969\n",
      "Test accuracy: 0.599799398195\n"
     ]
    }
   ],
   "source": [
    "# The actual training of the CNN using the parameters and model previously specified.\n",
    "# The validation set is a split of the stratified sampled training data.\n",
    "history2 = model2.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split = 0.15)\n",
    "\n",
    "# Evaluate the performance on the unused testing set.\n",
    "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score2[0])\n",
    "print('Test accuracy:', score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save_weights('tuned_weights2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x130cc9a90>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x130702e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x130702c50>,\n",
       " <keras.layers.core.Dropout at 0x130702210>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x130cc9ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x129737b50>,\n",
       " <keras.layers.core.Dropout at 0x13071d990>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x129ac4fd0>,\n",
       " <keras.layers.core.Flatten at 0x115c0e710>,\n",
       " <keras.layers.core.Dense at 0x115c1bbd0>,\n",
       " <keras.layers.core.Dropout at 0x129aadd90>,\n",
       " <keras.layers.core.Dense at 0x115c61890>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAFtCAYAAAAaiCMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIXCKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11974bd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here is a visualization of the training process\n",
    "# typically we gain a lot in the beginning and then\n",
    "# training slows down\n",
    "plt.plot(history2.history['acc'])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Training Accuracy, Additional Layers\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we see that this model with 250 epochs and nother convultional, dropout, and pooling layer  yielded an accuracy of ~60% on the unseen test set, and 75% accuracy on the training set.  This is a reduction of 20% accuracy on the training set and a 10% reduction on the test set, from the previous, simpler model.  It is believed that the addition of the convolutional layer and pooling layer are simplifing the details of the image excessively and loosing detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tune an existing CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model_tune = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model_tune.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x115826750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x115826450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11585fbd0>,\n",
       " <keras.layers.core.Activation at 0x11585fb10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1158762d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x115851d50>,\n",
       " <keras.layers.core.Activation at 0x115b69e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x115bd1e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x116010850>,\n",
       " <keras.layers.core.Activation at 0x11607c790>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1160a6810>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11606dc10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11610df10>,\n",
       " <keras.layers.core.Activation at 0x11617a410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x116301bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11616af50>,\n",
       " <keras.layers.core.Activation at 0x1163c8250>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x116326f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x116765ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1167c0b10>,\n",
       " <keras.layers.core.Activation at 0x1168c6990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1164ee610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x116942d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x116495350>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1168c6f90>,\n",
       " <keras.layers.core.Activation at 0x116723810>,\n",
       " <keras.layers.core.Activation at 0x1169737d0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x116fafe50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1163f7090>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11660dc90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x116f2bfd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x116fed550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1163e7450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1166b9dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11699ee50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x116f74a50>,\n",
       " <keras.layers.core.Activation at 0x1164b6c50>,\n",
       " <keras.layers.core.Activation at 0x1167d7190>,\n",
       " <keras.layers.core.Activation at 0x116f978d0>,\n",
       " <keras.layers.core.Activation at 0x1170b5610>,\n",
       " <keras.layers.merge.Concatenate at 0x11707bc90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1176015d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1174c5dd0>,\n",
       " <keras.layers.core.Activation at 0x1176e9610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x117180e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x117732fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1171b1c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1174e2c10>,\n",
       " <keras.layers.core.Activation at 0x117436c90>,\n",
       " <keras.layers.core.Activation at 0x1177f2190>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x117904ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x117163a10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1173eab10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1177d1a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1178e8ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x117156950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1173eac50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11784aad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1178d6c50>,\n",
       " <keras.layers.core.Activation at 0x11734fcd0>,\n",
       " <keras.layers.core.Activation at 0x117497fd0>,\n",
       " <keras.layers.core.Activation at 0x11790f6d0>,\n",
       " <keras.layers.core.Activation at 0x1179ff810>,\n",
       " <keras.layers.merge.Concatenate at 0x1179d0410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x119d8ac90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x119e03c90>,\n",
       " <keras.layers.core.Activation at 0x119f3d210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x117fca950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x119e73450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11828f990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x119e90a10>,\n",
       " <keras.layers.core.Activation at 0x118204a10>,\n",
       " <keras.layers.core.Activation at 0x119f8f490>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x119fc69d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x117a2a4d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x119cdc550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x119fb8a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11a1e4b10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x117fbbf10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1182ecfd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a16ce10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a1c9350>,\n",
       " <keras.layers.core.Activation at 0x117fdb950>,\n",
       " <keras.layers.core.Activation at 0x119daaf90>,\n",
       " <keras.layers.core.Activation at 0x11a267cd0>,\n",
       " <keras.layers.core.Activation at 0x11a2a7d90>,\n",
       " <keras.layers.merge.Concatenate at 0x11a2c5bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11a3e6d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a412090>,\n",
       " <keras.layers.core.Activation at 0x11a4f0c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11a500510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a569b10>,\n",
       " <keras.layers.core.Activation at 0x11a66f890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11a36c810>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11a5d8510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a3aef50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a59a990>,\n",
       " <keras.layers.core.Activation at 0x11a3f9090>,\n",
       " <keras.layers.core.Activation at 0x11a7a8a10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x11a641890>,\n",
       " <keras.layers.merge.Concatenate at 0x11a70bb90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11aca4810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ac0ab50>,\n",
       " <keras.layers.core.Activation at 0x11adcda90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11ac61910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ad2f8d0>,\n",
       " <keras.layers.core.Activation at 0x11aea2e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11a8afe90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11ae36a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a7f5fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ae36650>,\n",
       " <keras.layers.core.Activation at 0x11aa19890>,\n",
       " <keras.layers.core.Activation at 0x11ae1ab10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11aa33850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11b021650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a8ce4d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b011d90>,\n",
       " <keras.layers.core.Activation at 0x11ab13e10>,\n",
       " <keras.layers.core.Activation at 0x11b03cd90>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11b3c73d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11a786d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11aad6250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11b280dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11b30f890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11a7e6d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ab8ab90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b236950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b376210>,\n",
       " <keras.layers.core.Activation at 0x11a6ef490>,\n",
       " <keras.layers.core.Activation at 0x11ac19ad0>,\n",
       " <keras.layers.core.Activation at 0x11b33e150>,\n",
       " <keras.layers.core.Activation at 0x11b46f350>,\n",
       " <keras.layers.merge.Concatenate at 0x11b4bfb10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11b940c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b9bac90>,\n",
       " <keras.layers.core.Activation at 0x11baf2210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11ba2a450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ba46a10>,\n",
       " <keras.layers.core.Activation at 0x11bb41490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11b52ccd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11bb6fa90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b558a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11bbfbe10>,\n",
       " <keras.layers.core.Activation at 0x11b707d90>,\n",
       " <keras.layers.core.Activation at 0x11bcf5cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11b744950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11bb7a9d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b7de990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11bc74510>,\n",
       " <keras.layers.core.Activation at 0x11b751a10>,\n",
       " <keras.layers.core.Activation at 0x11bf0be90>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11bf9e3d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11b51e9d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11b893550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11bc585d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11bf53950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b4fe410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11b83bfd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11be7c890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11bfad610>,\n",
       " <keras.layers.core.Activation at 0x11b5c5c50>,\n",
       " <keras.layers.core.Activation at 0x11b962f90>,\n",
       " <keras.layers.core.Activation at 0x11befaa50>,\n",
       " <keras.layers.core.Activation at 0x11c089a50>,\n",
       " <keras.layers.merge.Concatenate at 0x11c0d5c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11c634fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c5789d0>,\n",
       " <keras.layers.core.Activation at 0x11c6a08d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11c6b9e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c653510>,\n",
       " <keras.layers.core.Activation at 0x11c794f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11c193e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11c785610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c1f9890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c7dfbd0>,\n",
       " <keras.layers.core.Activation at 0x11c435a10>,\n",
       " <keras.layers.core.Activation at 0x11ca28850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11c31f9d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11c99eb10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c398590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c9bcb10>,\n",
       " <keras.layers.core.Activation at 0x11c37a490>,\n",
       " <keras.layers.core.Activation at 0x11cb1f6d0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11cbbaa90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11c05cc10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11c4ac590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11ca87350>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11cbbac50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c138b50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11c49fb90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11cab4e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11cbd5610>,\n",
       " <keras.layers.core.Activation at 0x11c1cc650>,\n",
       " <keras.layers.core.Activation at 0x11c52d850>,\n",
       " <keras.layers.core.Activation at 0x11cd05c90>,\n",
       " <keras.layers.core.Activation at 0x11ce2c7d0>,\n",
       " <keras.layers.merge.Concatenate at 0x11cd95fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11d245e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11d29ea10>,\n",
       " <keras.layers.core.Activation at 0x11d282950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11d4b2610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11d476e10>,\n",
       " <keras.layers.core.Activation at 0x11d59e650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11cf051d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11d5e5ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11cf35f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11d493c50>,\n",
       " <keras.layers.core.Activation at 0x11d0abf90>,\n",
       " <keras.layers.core.Activation at 0x11d6a42d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11d073fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11d684ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11d085e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11d6ffc50>,\n",
       " <keras.layers.core.Activation at 0x11d202710>,\n",
       " <keras.layers.core.Activation at 0x11d8381d0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11da32e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11b4cfa10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11d1d24d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11d7aec50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11d8b4ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ce80fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11d197d10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11d78ea90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11d8ede50>,\n",
       " <keras.layers.core.Activation at 0x11ce3fe90>,\n",
       " <keras.layers.core.Activation at 0x11d42aa90>,\n",
       " <keras.layers.core.Activation at 0x11d849150>,\n",
       " <keras.layers.core.Activation at 0x11dabc390>,\n",
       " <keras.layers.merge.Concatenate at 0x11da71fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11dc7d410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11dce2050>,\n",
       " <keras.layers.core.Activation at 0x11dec2f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11df0b910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11dca9cd0>,\n",
       " <keras.layers.core.Activation at 0x11e13c850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11db05410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11defa210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11db93f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11dfb7250>,\n",
       " <keras.layers.core.Activation at 0x11da9e450>,\n",
       " <keras.layers.core.Activation at 0x11e259d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11dbec350>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11dfe2790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11dba54d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11e1dc550>,\n",
       " <keras.layers.core.Activation at 0x11dcc87d0>,\n",
       " <keras.layers.core.Activation at 0x11e2c6310>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x11e1bf610>,\n",
       " <keras.layers.merge.Concatenate at 0x11e2e2510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11e88d7d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11e8acdd0>,\n",
       " <keras.layers.core.Activation at 0x11e8f8d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11e405850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11e908510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11e3bc990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11e972b10>,\n",
       " <keras.layers.core.Activation at 0x11e4e2e10>,\n",
       " <keras.layers.core.Activation at 0x11eb79890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11e4a8250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11e775810>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11e9e4510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11eb49890>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11eddab90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11e3a0b50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11e65db90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11e6dbb50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11e9a3990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ec15590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11edcbad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11e361d50>,\n",
       " <keras.layers.core.Activation at 0x11e6eaad0>,\n",
       " <keras.layers.core.Activation at 0x11e89da90>,\n",
       " <keras.layers.core.Activation at 0x11ecb2a10>,\n",
       " <keras.layers.core.Activation at 0x11ebf8490>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11ed1da50>,\n",
       " <keras.layers.core.Activation at 0x11e3eb890>,\n",
       " <keras.layers.merge.Concatenate at 0x11e735910>,\n",
       " <keras.layers.merge.Concatenate at 0x11ed2b590>,\n",
       " <keras.layers.core.Activation at 0x11ee3ee50>,\n",
       " <keras.layers.merge.Concatenate at 0x11ee7d550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11f344ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11f3fc950>,\n",
       " <keras.layers.core.Activation at 0x11f323590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11ee0ab50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11f473350>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11f03d890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11f42a4d0>,\n",
       " <keras.layers.core.Activation at 0x11f01fa10>,\n",
       " <keras.layers.core.Activation at 0x11f6507d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11f15b3d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11f2c6550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11f606410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11f791910>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x11f919fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11ee5f410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11f112cd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11f266fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11f66a050>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11f631cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11f938dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11eefef50>,\n",
       " <keras.layers.core.Activation at 0x11f23d850>,\n",
       " <keras.layers.core.Activation at 0x11f24ca90>,\n",
       " <keras.layers.core.Activation at 0x11f745f90>,\n",
       " <keras.layers.core.Activation at 0x11f8c7850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x11f83ef50>,\n",
       " <keras.layers.core.Activation at 0x11f0c7e90>,\n",
       " <keras.layers.merge.Concatenate at 0x11f332c50>,\n",
       " <keras.layers.merge.Concatenate at 0x11f782210>,\n",
       " <keras.layers.core.Activation at 0x11f9b0ad0>,\n",
       " <keras.layers.merge.Concatenate at 0x11f973b50>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x115826810>,\n",
       " <keras.layers.core.Dense at 0x1158268d0>,\n",
       " <keras.layers.core.Dense at 0x11fa6de90>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tune.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected dense_4 to have shape (None, 200) but got array with shape (4996, 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f23001868316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_split = 0.15)\n\u001b[0m",
      "\u001b[0;32m/Users/ted/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1407\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ted/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1298\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m                                     exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1301\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1302\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/Users/ted/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    131\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: expected dense_4 to have shape (None, 200) but got array with shape (4996, 7)"
     ]
    }
   ],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "# fine-tune the model\n",
    "tune_history = model_tune.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:172]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
