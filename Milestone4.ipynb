{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109B Advanced Topics in Data Science, Final Project, Milestone 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 9 - Steve Robbins, Chad Tsang, and Ted Heuer\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Due Date: ** Wednesday, April 12th, 2017 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 4: Deep learning, due Wednesday, April 26, 2017\n",
    "\n",
    "For this milestone you will (finally) use deep learning to predict movie genres. You will train one small network from scratch on the posters only, and compare this one to a pre-trained network that you fine tune. [Here](https://keras.io/getting-started/faq/#how-can-i-use-pre-trained-models-in-keras) is a description of how to use pretrained models in Keras.\n",
    "\n",
    "You can try different architectures, initializations, parameter settings, optimization methods, etc. Be adventurous and explore deep learning! It can be fun to combine the features learned by the deep learning model with a SVM, or incorporate meta data into your deep learning model. \n",
    "\n",
    "**Note:** Be mindful of the longer training times for deep models. Not only for training time, but also for the parameter tuning efforts. You need time to develop a feel for the different parameters and which settings work, which normalization you want to use, which model architecture you choose, etc. \n",
    "\n",
    "It is great that we have GPUs via AWS to speed up the actual computation time, but you need to be mindful of your AWS credits. The GPU instances are not cheap and can accumulate costs rather quickly. Think about your model first and do some quick dry runs with a larger learning rate or large batch size on your local machine. \n",
    "\n",
    "The notebook to submit this week should at least include:\n",
    "\n",
    "- Complete description of the deep network you trained from scratch, including parameter settings, performance, features learned, etc. \n",
    "- Complete description of the pre-trained network that you fine tuned, including parameter settings, performance, features learned, etc. \n",
    "- Discussion of the results, how much improvement you gained with fine tuning, etc. \n",
    "- Discussion of at least one additional exploratory idea you pursued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links - Delete or cite these.\n",
    "https://keras.io/callbacks/\n",
    "\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "https://keras.io/getting-started/faq/#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory\n",
    "\n",
    "https://elitedatascience.com/keras-tutorial-deep-learning-in-python#step-4\n",
    "\n",
    "http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install keras \n",
    "#!pip install tensorflow\n",
    "#!pip install tensorflow.python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, split, and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_poster_data(image_size, source_size = 'w92', verbose = False):\n",
    "    # Loads the poster image data at the requested size, the assigned genre, and the movie id.\n",
    "    #\n",
    "    y_labels = pd.read_csv('y_labels_multiclass.csv')\n",
    "    image_path = './posters/' + source_size + '/'\n",
    "    posters = pd.DataFrame()\n",
    "    for movie in y_labels.iterrows():\n",
    "        row = movie[0]\n",
    "        movie_id = movie[1]['movie_id']\n",
    "        genre_id = int(movie[1]['genre_id'].replace('[', '').replace(']',''))\n",
    "        try:\n",
    "            image = misc.imread(image_path + str(movie_id) + '.jpg')\n",
    "            image_resize = img_to_array(misc.imresize(image, image_size))\n",
    "            if (image_resize.shape[2]==3):\n",
    "                posters = posters.append({'movie_id' : movie_id, \n",
    "                                          'genre_id' : genre_id,\n",
    "                                          'poster' : image_resize}, ignore_index = True)\n",
    "        except IOError:\n",
    "            if (verbose == True):\n",
    "                print('Unable to load poster for movie #', movie_id)\n",
    "    print('Loaded ', posters.shape[0], ' posters.')\n",
    "    return posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stratified_sampler(dataset, observations):\n",
    "    # Performs a stratified sample on the dataset and returns the number of observations \n",
    "    # requested.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #    dataset:  The dataframe to sample, observing class relationships.\n",
    "    #    observations:  The number of total target observations across all classes.\n",
    "    #\n",
    "    # Returns:\n",
    "    #    A pandas dataframe sampled from the dataset maintaining class relationships.\n",
    "    class_weights = dataset.groupby(\"genre_id\").agg(['count'])/len(dataset)\n",
    "    class_sample_counts = class_weights * observations\n",
    "    class_count = class_weights.shape[0]\n",
    "    sampled = pd.DataFrame()\n",
    "    for class_to_sample in class_sample_counts.iterrows():\n",
    "        class_name = class_to_sample[0]\n",
    "        desired_class_observations = class_to_sample[1][0]\n",
    "        sampled_obs = dataset[dataset[\"genre_id\"]==class_name].sample(int(desired_class_observations), replace=\"True\")\n",
    "        sampled = sampled.append(sampled_obs, ignore_index=True)\n",
    "    return sampled, class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reshape_and_normalize(data):\n",
    "    image_count = data.shape[0]\n",
    "    temp = np.ndarray(shape=(image_count, data[0].shape[0], data[0].shape[1], 3))\n",
    "\n",
    "    for index in range(0, image_count):\n",
    "        try:\n",
    "            temp[index] = data[index].reshape(data[0].shape[0], data[0].shape[1], 3)\n",
    "        except ValueError:\n",
    "            print(data[index].shape)\n",
    "    temp = temp.astype('float32')\n",
    "    temp /= 255.0\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_responses(data):\n",
    "    unique_responses = np.sort(data[\"genre_id\"].unique())\n",
    "    data[\"genre_id\"] = data[\"genre_id\"].replace(unique_responses, range(0,len(unique_responses)), inplace=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_split_prepare_data(train_observations, test_observations, image_size, sample = 'stratified'):\n",
    "    # Loads, splits, and prepares the data for use by a CNN model.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #    train_observations:  The dataframe to sample, observing class relationships.\n",
    "    #    test_observations:  The number of total target observations across all classes.\n",
    "    #    sample:  The sampling method, currently only supports 'stratified'\n",
    "    #\n",
    "    # Returns:\n",
    "    #    Nothing.\n",
    "    posters_data = load_poster_data(image_size)\n",
    "    posters = normalize_responses(posters_data)\n",
    "    \n",
    "    if (sample == 'stratified'):\n",
    "        train_sample, class_count_train = stratified_sampler(posters, train_observations)\n",
    "        test_sample, class_count_test = stratified_sampler(posters, test_observations)\n",
    "    else:\n",
    "        raise('Unsupported sample method : ', sample)\n",
    "         \n",
    "    x_train = train_sample[\"poster\"]\n",
    "    y_train = train_sample[\"genre_id\"]\n",
    "    x_test = test_sample[\"poster\"]\n",
    "    y_test = test_sample[\"genre_id\"]\n",
    "\n",
    "    img_rows = x_train[0].shape[0]\n",
    "    img_cols = x_train[0].shape[1]\n",
    "    print('Classes : ', class_count_train)\n",
    "        \n",
    "    x_train = reshape_and_normalize(x_train)\n",
    "    x_test = reshape_and_normalize(x_test)\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    # Convert response to one hot encoding\n",
    "    y_train = keras.utils.to_categorical(y_train, class_count_train)\n",
    "    y_test = keras.utils.to_categorical(y_test, class_count_test)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test), class_count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  6824  posters.\n",
      "Classes :  7\n",
      "x_train shape: (4996, 138, 92, 3)\n",
      "4996 train samples\n",
      "997 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), classes = load_split_prepare_data(train_observations = 5000, \n",
    "                                                                        test_observations = 1000, \n",
    "                                                                        image_size = (138,92), \n",
    "                                                                        sample='stratified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Convolutional Neural Net architecture, from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = 7\n",
    "final_activation_function = 'softmax'\n",
    "\n",
    "input_activation_function = 'relu'\n",
    "input_kernel_size = (5,5)\n",
    "input_shape = (138, 92, 3)\n",
    "pool_size = (3,3)\n",
    "\n",
    "hidden_activation_function = 'relu'\n",
    "hidden_kernel_size = (3,3)\n",
    "\n",
    "loss_method = 'categorical_crossentropy'\n",
    "optimizer = SGD(lr=0.1, momentum=0.9)\n",
    "eval_metric = 'accuracy'\n",
    "\n",
    "# smaller batch size means noisier gradient, but more updates per epoch\n",
    "batch_size = 256\n",
    "# number of iterations over the complete training data\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 134, 88, 16)       1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 44, 29, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 42, 27, 32)        4640      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 42, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 14, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 4032)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                258112    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 264,423\n",
      "Trainable params: 264,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an empty network model\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Conv2D(16, kernel_size=input_kernel_size, activation=input_activation_function, input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Hidden Layer(s)\n",
    "model.add(Conv2D(32, kernel_size=hidden_kernel_size, activation=hidden_activation_function))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Adding another layer did not improve performance, perhaps because of the pooling on pooling.\n",
    "#model.add(Conv2D(48, kernel_size=hidden_kernel_size, activation=hidden_activation_function))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Classification layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=hidden_activation_function))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes, activation=final_activation_function))\n",
    "\n",
    "# Display the CNN.\n",
    "model.summary()\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(loss=loss_method, optimizer=optimizer, metrics=[eval_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4246 samples, validate on 750 samples\n",
      "Epoch 1/40\n",
      "4246/4246 [==============================] - 47s - loss: 1.6231 - acc: 0.2756 - val_loss: 4.0526 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "4246/4246 [==============================] - 46s - loss: 1.7014 - acc: 0.2765 - val_loss: 3.0914 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "4246/4246 [==============================] - 45s - loss: 1.5846 - acc: 0.2892 - val_loss: 4.4150 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "4246/4246 [==============================] - 45s - loss: 1.5365 - acc: 0.3121 - val_loss: 5.2125 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "4246/4246 [==============================] - 44s - loss: 1.4745 - acc: 0.3860 - val_loss: 5.5264 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "4246/4246 [==============================] - 45s - loss: 1.4472 - acc: 0.4013 - val_loss: 5.5061 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "4246/4246 [==============================] - 44s - loss: 1.4315 - acc: 0.4105 - val_loss: 5.6866 - val_acc: 0.0000e+00\n",
      "Epoch 8/40\n",
      "4246/4246 [==============================] - 44s - loss: 1.4135 - acc: 0.4093 - val_loss: 5.8210 - val_acc: 0.0000e+00\n",
      "Epoch 9/40\n",
      "4246/4246 [==============================] - 45s - loss: 1.4122 - acc: 0.4199 - val_loss: 5.8172 - val_acc: 0.0000e+00\n",
      "Epoch 10/40\n",
      "4246/4246 [==============================] - 44s - loss: 1.3930 - acc: 0.4315 - val_loss: 6.0743 - val_acc: 0.0000e+00\n",
      "Epoch 11/40\n",
      "4246/4246 [==============================] - 44s - loss: 1.3791 - acc: 0.4385 - val_loss: 6.0443 - val_acc: 0.0000e+00\n",
      "Epoch 12/40\n",
      "4246/4246 [==============================] - 44s - loss: 1.3792 - acc: 0.4409 - val_loss: 5.9445 - val_acc: 0.0000e+00\n",
      "Epoch 13/40\n",
      "4246/4246 [==============================] - 44s - loss: 1.3592 - acc: 0.4454 - val_loss: 5.9810 - val_acc: 0.0000e+00\n",
      "Epoch 14/40\n",
      "4246/4246 [==============================] - 57s - loss: 1.3654 - acc: 0.4439 - val_loss: 6.0717 - val_acc: 0.0000e+00\n",
      "Epoch 15/40\n",
      "4246/4246 [==============================] - 59s - loss: 1.3516 - acc: 0.4548 - val_loss: 6.2060 - val_acc: 0.0000e+00\n",
      "Epoch 16/40\n",
      "4246/4246 [==============================] - 59s - loss: 1.3304 - acc: 0.4654 - val_loss: 6.0193 - val_acc: 0.0000e+00\n",
      "Epoch 17/40\n",
      "4246/4246 [==============================] - 60s - loss: 1.3226 - acc: 0.4616 - val_loss: 6.1790 - val_acc: 0.0000e+00\n",
      "Epoch 18/40\n",
      "4246/4246 [==============================] - 60s - loss: 1.3055 - acc: 0.4746 - val_loss: 6.2657 - val_acc: 0.0000e+00\n",
      "Epoch 19/40\n",
      "4246/4246 [==============================] - 60s - loss: 1.2983 - acc: 0.4696 - val_loss: 6.2203 - val_acc: 0.0000e+00\n",
      "Epoch 20/40\n",
      "4246/4246 [==============================] - 60s - loss: 1.2810 - acc: 0.4729 - val_loss: 6.1247 - val_acc: 0.0000e+00\n",
      "Epoch 21/40\n",
      "4246/4246 [==============================] - 45s - loss: 1.2774 - acc: 0.4880 - val_loss: 6.4455 - val_acc: 0.0000e+00\n",
      "Epoch 22/40\n",
      "4246/4246 [==============================] - 49s - loss: 1.2782 - acc: 0.4870 - val_loss: 6.3816 - val_acc: 0.0000e+00\n",
      "Epoch 23/40\n",
      "4246/4246 [==============================] - 52s - loss: 1.2550 - acc: 0.4842 - val_loss: 6.4467 - val_acc: 0.0000e+00\n",
      "Epoch 24/40\n",
      "4246/4246 [==============================] - 52s - loss: 1.2277 - acc: 0.5054 - val_loss: 6.6319 - val_acc: 0.0000e+00\n",
      "Epoch 25/40\n",
      "4246/4246 [==============================] - 51s - loss: 1.2240 - acc: 0.5021 - val_loss: 6.5496 - val_acc: 0.0000e+00\n",
      "Epoch 26/40\n",
      "4246/4246 [==============================] - 54s - loss: 1.2101 - acc: 0.5148 - val_loss: 6.7678 - val_acc: 0.0000e+00\n",
      "Epoch 27/40\n",
      "4246/4246 [==============================] - 50s - loss: 1.1823 - acc: 0.5148 - val_loss: 6.8953 - val_acc: 0.0000e+00\n",
      "Epoch 28/40\n",
      "4246/4246 [==============================] - 55s - loss: 1.1721 - acc: 0.5240 - val_loss: 7.0897 - val_acc: 0.0000e+00\n",
      "Epoch 29/40\n",
      "4246/4246 [==============================] - 49s - loss: 1.1714 - acc: 0.5231 - val_loss: 6.9251 - val_acc: 0.0000e+00\n",
      "Epoch 30/40\n",
      "4246/4246 [==============================] - 71s - loss: 1.1409 - acc: 0.5367 - val_loss: 7.0709 - val_acc: 0.0000e+00\n",
      "Epoch 31/40\n",
      "4246/4246 [==============================] - 63s - loss: 1.1329 - acc: 0.5311 - val_loss: 7.2062 - val_acc: 0.0000e+00\n",
      "Epoch 32/40\n",
      "4246/4246 [==============================] - 48s - loss: 1.1442 - acc: 0.5268 - val_loss: 6.8355 - val_acc: 0.0000e+00\n",
      "Epoch 33/40\n",
      "4246/4246 [==============================] - 47s - loss: 1.1280 - acc: 0.5407 - val_loss: 7.2972 - val_acc: 0.0000e+00\n",
      "Epoch 34/40\n",
      "4246/4246 [==============================] - 47s - loss: 1.1088 - acc: 0.5433 - val_loss: 7.3105 - val_acc: 0.0000e+00\n",
      "Epoch 35/40\n",
      "4246/4246 [==============================] - 46s - loss: 1.0693 - acc: 0.5624 - val_loss: 7.3163 - val_acc: 0.0000e+00\n",
      "Epoch 36/40\n",
      "4246/4246 [==============================] - 47s - loss: 1.0640 - acc: 0.5601 - val_loss: 7.4993 - val_acc: 0.0000e+00\n",
      "Epoch 37/40\n",
      "4246/4246 [==============================] - 48s - loss: 1.0740 - acc: 0.5612 - val_loss: 7.5037 - val_acc: 0.0000e+00\n",
      "Epoch 38/40\n",
      "4246/4246 [==============================] - 46s - loss: 1.0834 - acc: 0.5617 - val_loss: 7.3900 - val_acc: 0.0000e+00\n",
      "Epoch 39/40\n",
      "4246/4246 [==============================] - 51s - loss: 1.0701 - acc: 0.5622 - val_loss: 7.6975 - val_acc: 0.0000e+00\n",
      "Epoch 40/40\n",
      "4246/4246 [==============================] - 49s - loss: 1.0386 - acc: 0.5784 - val_loss: 7.5721 - val_acc: 0.0000e+00\n",
      "Test loss: 2.13424021263\n",
      "Test accuracy: 0.437311935807\n"
     ]
    }
   ],
   "source": [
    "# The actual training of the CNN using the parameters and model previously specified.\n",
    "# The validation set is a split of the stratified sampled training data.\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split = 0.15)\n",
    "\n",
    "# Evaluate the performance on the unused testing set.\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1192edf90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAFkCAYAAADIefl6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8VHW+//HXzKRn0iAhoaRQQw9GIooCKuKKooAuTZqa\nta7lesVdbEgxgm3vz1W4lqu4oi6uYmVXQRTFRaQYEjaBEEoIBEhIIG1SJ5nz+wMZQZBQMplk8n4+\nHjyYmTPfmc/hQN6c7/me79dkGIaBiIiIeCSzuwsQERER11HQi4iIeDAFvYiIiAdT0IuIiHgwBb2I\niIgHU9CLiIh4MC9XfbDD4WD27NlkZ2fj7e1NSkoKMTExzu1btmzhmWeewTAMwsPDef755/Hy8jpt\nGxERETk7Lgv6VatWYbfbWbp0Kenp6SxYsIBFixYBYBgGs2bN4qWXXiI6OpoPPviA/fv3s2PHjt9s\nIyIiImfPZV33qampDBkyBICEhAQyMjKc23JycggNDWXx4sVMnTqV0tJSOnfufNo2IiIicvZcdkZv\ns9mwWq3O5xaLBYfDgdlspri4mM2bNzNr1ixiYmK488476du372nbnEp1dTUZGRlERERgsVhctSsi\nIiLNQn19PYWFhfTt2xc/P78zauOyoLdarVRUVDifHx/YoaGhxMTE0KVLFwCGDBlCRkbGaducSkZG\nBpMnT3bRHoiIiDRP7777LgMHDjyj97os6BMTE1m9ejUjR44kLS2N+Ph457bo6GgqKyvZu3cvMTEx\n/PTTT/z+978nJibmN9ucSkREBHB0h6Oioly1KyIiIs1Cfn4+kydPdubfmXBZ0I8YMYK1a9cyceJE\nAObPn8/y5cuprKxk/PjxpKSk8NBDD2EYBomJiQwbNgzDME5qczrHuuujoqLo1KmTq3ZFRESkWTmb\ny9Wmlrx6XV5eHsOHD+frr79W0IuIiMc7l9zThDkiIiIeTEEvIiLiwRT0IiIiHkxBLyIi4sEU9CIi\nIh5MQS8iIuLBFPQiIiIeTEEvIiLiwRT0IiIiHkxBLyIi4sEU9CIiIh5MQS8iIuLBFPQiIiIeTEEv\nIiLiZg6H6xaSVdCLiIi4iWEYvPSPNG57aiX2OodLvkNBLyIi4iaf/3s3K9fn0i4sAC+LySXfoaAX\nERFxg8zdh3nzs0xCrb78edpATCYFvYiIiEc4UlbNM29vxAD+NG0gbUP8XfZdCnoREZEmVFfv4Jm3\nN1JcXsOto3rTr2u4S79PQS8iItKEFn+eydacI1yW0IHRQ7u6/PsU9CIiIk3ku9Q8Pvt+N9GRVu6f\ncIHLrssfT0EvIiLSBPYcLOOlD9Lw9/Xi0Vsuwt/Xq0m+V0EvIiLiYhVVdp5+awM1tfU8OOkCOrUL\narLvVtCLiIi4kMNh8D9/T+VgUQU3XdGNS/p1aNLvV9CLiIi40Iff7GB9Zj4J3cOZOrJXk3+/gl5E\nRMRFUrcf4p0vtxEe6s/DUwZisTR97CroRUREXKDgSCXPv7MJi9nMI9OTCLH6uqUOBb2IiEgjq7XX\ns+BvGyivtHPn2H70iAlzWy1NM7ZfRETkHH2zaR8LP0gjsm0gnTsE07lDCF06hNC5QzBhwX7n/fn1\n9Q5q7PXU2o/9Xk9NbT019voTntf+6vkv2x1Ht9f98npxWTUHiioYcVEMv7s4thH+FM6dgl5ERJqt\nQ8WVvPLRFgCKSqrYV1DOms37ndtDg3zp3P5o+HfueDT8O0VYsdc5KLHVUFxWQ3F59YmPy2soKf/l\nca0Llof18bZwYc923Hlj/yaZFOd0FPQiItIsHVurvaqmjgcmXMCVA6M5VFxJzoFSdu8vI+dAKTkH\nStmcXcjm7EJnO7MJHMbpP9vLYiI0yI+Y9sEE+Hrh423B18eCr/fRXz7eFny8zSe95uvz8+/Hfh33\n/Nh2b4sZs9m94X48Bb2IiDRLK9fnkpZdyMBekQxPisZkMhHVNpCotoEn3Ituq7Kz50Apuw+UsudA\nGXmHbPj7ehEa5EtYkC+hQX6EBfkSFuxLqNWXsGA/rP7ebj/TbioKehERaXYOFVfyxmeZBPp5ce+4\nhNOGstXfm75dw+nr4lXgWiqNuhcRkWbl+C77P4zu59K12lsDBb2IiDQrv+6yl/OjoBcRkWbjbLrs\n5cy47Bq9w+Fg9uzZZGdn4+3tTUpKCjExMc7tb731Fh9++CFhYUcnEZg3bx5xcXGMHTsWq9UKQHR0\nNE8//bSrShQRkWbk16Ps1WXfOFwW9KtWrcJut7N06VLS09NZsGABixYtcm7PzMzk2WefpXfv3s7X\nampqAFiyZImryhIRkWZqxY/qsncFl3Xdp6amMmTIEAASEhLIyMg4YXtmZiavvPIKN998M6+99hoA\nWVlZVFVVkZyczPTp00lPT3dVeSIi0owcOlLJm59nqMveBVx2Rm+z2Zxd8AAWiwWHw4HZfPT/Ftdd\ndx2TJ08mMDCQe++9l2+//ZYOHTqQnJzMuHHj2LNnD7fffjsrVqxwthEREc/zS5d9vbrsXcBlQW+1\nWqmoqHA+Pz7kAaZPn+78j8CwYcPYunUrgwcPJjb26JzAcXFxhIaGUlhYSGRkpKvKFBERN1vxYy5p\nO9Rl7youO1VOTExkzZo1AKSlpREfH+/cVl5ezqhRo6isrMQwDH788Uf69u3LsmXLWLBgAQAFBQXY\nbDYiIiJcVaKIiLiZuuxdz2Vn9CNGjGDt2rVMnDgRgPnz57N8+XIqKysZP348Dz74INOmTcPHx4fB\ngwczdOhQ7HY7M2fO5Oabb8ZkMjF//nx124uIeKjju+z/a6K67F3FZUFvMpmYM2fOCa917tzZ+Xj0\n6NGMHj36hO3e3t688MILripJRESakS+P67K/cqC67F1Fc92LiEiTcjgMdu8vZbG67JuEgl5ERFzG\nMAzyD1eyc18JO/JK2LmvhJ15JVTV1AGoy74JKOhFRKRRGIZBYUkVO/b9HOg/h7qtyu58j8kEndpZ\n6dYplAvi23F5Yic3Vtw6KOhFROS82evqefqtjWzaVnDC6+3DA0mMb0e36FC6RYfStWMIAX7ebqqy\ndVLQi4jIeal3GLzwbiqbthUQHxvGxX3b071TKF07hWAN8HF3ea2egl5ERM6ZYRi8+tEW1m45QJ8u\nbZlzxyX4elvcXZYcRzepi4i0cLX2ev75793s3FfS5N/93ortfLFuD507BPPEbYMU8s2QzuhFRFqw\n+noHzy7ZxPrMfAD6dGnL6KFduahPFBaza29Z+/z73Sz9ajvt2wYy5/ZLCPTXtffmSEEvItJCORwG\nf/1HGusz8+nTpS2+PhZSsw6Rufsw7dsGcsPQLgxPisHft/F/1H+Xmsdrn/yHsCBf5t55CWHBfo3+\nHdI4FPQiIi2QYRj832cZfLNpHz1iQpmVPIgAP29y88v4bM1uVv+0j1c//g/vfJnFNRfHMuqyLoSH\nNs796qlZh/ifv6cS6OfFnDsuIaptYKN8rriGgl5EpAVaunI7n3+/m5ioIJ78wyXOW9Zio4K5b/wA\npo7sxRfr9vCvtTksW72TT77bxaUJHRgzrCvdo8PO+Xu35x7h6b9twGI28UTyxXTuENJIeySuoqAX\nEWlhPvt+F++t3E5kmwDm3nEJwYEn38IWGuTLpKvjuemKbqzZnMcn3+1izeb9rNm8n96d23DNJXFc\n1DvqrK6r780vY87//Yi9zsGj05Po06VtY+6WuIiCXkSkBflm015e/ySDsCBf5t05uMHpY328LVx1\nUSzDk2JIyy7kkzW7SM06xNacI3hZzCTGt+OyAR0aDP1DxZU8+do6yivtPDDhAgb1bd/YuyYuoqAX\nEWkhfsw4yIvvp2H192bunYNpH37m18ZNJhMXxLfjgvh27C+08X3aftamH2DD1nw2bM0/beiX2mqY\n9eo6ikqruXVUb666KMYVuycuoqAXEWkBtuws5Nklm/D2MvPk7RcT1z74nD+rY4SViSPimTginn0F\n5azdcuA3Q79/t3BSFm9gf6GNGy/vxo1XdG/EvZKmoKAXEWnmsvcW89Sb6zEMeOyWi+gZ26bRPjs6\nMui0oX/MVUkx3DKqd6N9rzQdBb2ISDO2N7+M2a+vo6a2nj9PS+KC+HYu+65Thf66/xwkJipIa8a3\nYAp6EZEmUlZRS8auIvx9vQgK9CE4wIegQB/8fCynDNGCI5U88erRAXD3jx/A4P4dmqzW40NfWjYF\nvYhIE9i8/egkM8XlNSdt87KYCQ70Jujn4A8K8CE40If0HYUcKasm+YY+jBgU64aqxRMo6EVEXMhe\n52DJF9v4+NudWMwmbry8G/5+XpRX1FJWWUt5RS22SjtllbUcLq0mN7/8hPbjr+rBmGHd3FS9eAIF\nvYiIixwotPHcO5vYmVdKh/BAZky5sMFZ6eodBrbKWsorazGbTHSIsDZRteKpFPQiIo3MMAy+3riP\nVz/eQnVtPVclxXDH2H5ntLiMxWwixOpLiNW3CSqV1kBBLyLSiGxVdv73w3TWpO0nwM+Lh6dcyNAL\nOrm7LGnFFPQiIo1kW84Rnn93E4eKq+gZG8aMKQOJbBPg7rKklVPQi4icp3qHwQdfZ/P3ldvBMJgw\nogeTRsRjsZjdXZqIgl5E5NcKi6vIOVB6Ru91GAaffLeLzN2HCQ/156GbE+nbNdzFFYqcOQW9iMhx\nMncfZvbr66iurT+rdoP7t+e+cQOwBpy8ZKyIOynoRUR+dizk7XUOJozoQaDfma3V3j48kEF9ojRF\nrDRLCnoREU4M+T9PS+KSflpvXTyDRoqISKunkBdPpqAXkVZNIS+eTkEvIq2WQl5aAwW9iLRKCnlp\nLRT0ItJiGYZxTu0ydhUp5KXV0Kh7EWmR9hws488vf0+o1Zf+3SPo3y2c/t3CG1wMJmNXEXP+70eF\nvLQaCnoRaXEMw+B/l6VTWV2HYRh8uW4PX67bA0Bc+2D6dw8noXsEfbu0JeC4e+EV8tIauSzoHQ4H\ns2fPJjs7G29vb1JSUoiJiXFuf+utt/jwww8JCzu6NvO8efOIjY3lySef/M02IiIAq3/KY2vOES7p\n154/Tx3IzrwS0ncUsWVnIdtyjrDnYBmfrdmN2Wyie6dQ+ncPJ6ptIK9/8h+FvLQ6Lgv6VatWYbfb\nWbp0Kenp6SxYsIBFixY5t2dmZvLss8/Su3dv52srV648bRsRkYoqO4uXZ+LjbeEPN/TFYjETH9uG\n+Ng2jL+qB7X2erbnFpO+o5AtO4vYvreY7XuLgaNrvSvkpbVxWdCnpqYyZMgQABISEsjIyDhhe2Zm\nJq+88gpFRUVcfvnl3HHHHQ22ERF5b0UWJeU1TBnZk3anWALWx9tCv27h9Ot2dGGZymo7W3OOkLn7\nMP27hXNBfLumLlnErVwW9DabDavV6nxusVhwOByYzUcH+l933XVMnjyZwMBA7r33Xr799tsG24hI\n65ZzoJTl/95N+/BAbry82xm1CfDzZmCvSAb2inRxdSLNk8uC3mq1UlFR4Xz+68CePn26M9SHDRvG\n1q1bG2wjIq2XYRi88tEWHAbcObYf3l4Wd5ck0iK4LEUTExNZs2YNAGlpacTHxzu3lZeXM2rUKCor\nKzEMgx9//JG+ffueto2ItG7HD8C7sKfOzkXOlMvO6EeMGMHatWuZOHEiAPPnz2f58uVUVlYyfvx4\nHnzwQaZNm4aPjw+DBw9m6NChGIZxUhsREVuVncWf/zIAT0TOnMuC3mQyMWfOnBNe69y5s/Px6NGj\nGT16dINtRETeW5FFia2GqSN7nXIAnoj8Nl0AF5FmLedAKf/89246hAcy9vKu7i5HpMVR0ItIs3V0\nBrxjA/D6awCeyDlQ0ItIs7X6p31s23N0AF5iT93/LnIuFPQi0iwdHYC39egAvNEagCdyrhT0ItIs\nHRuAN3FED9qFaQCeyLnS6nUicta+/Wkf32zaR4C/N8EBPgQF+hAU4E3Qz49/ec2HQH9vLGbTWX3+\n8QPwxgzTADyR86GgF5GzsiEzn7/8PRXDOLP3m0wQavWlW3QoPWPb0DMujO7RYfj7nvrHj8OhAXgi\njUlBLyJnLOdAKc+9swlvLwspdw8mMiyAsspayitqKa+spazCTvlxz4/+snOouJKNWwvYuLUAALMJ\n4tqHEB8bRs+4MHrGtqF9eCAmk8k5AG9wfw3AE2kMCnoROSNHyqqZ+8Z6qmvreWR6Ej1j2wAQFux3\nxu2z9hwhK7eYrD1H2JlXwu4DpXyxbg8AwYE+xMeGkb23GF8fC8maAU+kUSjoRaRBNfZ6nnpzPUUl\nVUy7theD+3c4689oE+zH4P4dnG3tdQ5yDpSSlXuE7XuKyco94jzjn3ZtLw3AE2kkCnoROS2Hw+B/\n/p7Kjn0lXDkwmt9f2b1RPtfby0yPmDB6xITBkKOvHSmr5mBRBb3i2jTKd4iIgl5EGvDeiizWph+g\nT5e23DsuAZPp7EbQn402wX60OcNLASJyZnQfvYj8ptU/7eP9VdlEtQ3gkelJGgEv0gIp6EXklLbm\nHOav76cR6OfFrOSLCbH6urskETkHCnoROUn+4QpSFm/AYRjMnJ5EdGSQu0sSkXOkoBeRE1RU2Zn7\nxnrKKmq5a2w/BvTQvewiLZmCXkSc6usdPPP2RvYVlHPD0C6MHNzZ3SWJyHnSqHuRFiR7bzHz/7aR\nDuGBjBgUy+B+7fHxbrwBcq9/msHm7EIG9orktus1YY2IJ1DQi7QQO/NKmPXaOiqr7RSVVLFlZxGv\n+HtzRWInRgyKpUvHkHP6XIfDYG9BOas37eOfa3OIax/Mw1MuPOuFaESkeVLQi7QAu/eX8sQrP1BZ\nbee/JyXSIyaMrzbs5euNe1m+Nofla3Po1imEEYNiGXpBJ6z+3r/5WYZhcPBwBVt2FJG+o5D/7Cqi\n1FYLQGiQL0/cNogAv99uLyIti4JepJnbc7CMx1/5gYpqOw9MuIDLL4wGYPp1vZlyTU82bStg5fq9\nbMoq4H+XbeGNzzK5tH97rh4US58ubTGZTBwurSJ9RxFbdhaSvqOIopIq5+e3Cfbjigs70b9bBEm9\nI3UbnYiHUdCLNGN788t4/JW1lFfWct/4AQxPijlhu8ViZlDf9gzq257DpVV8s2kfX63fy+qf8lj9\nUx4dwgMxmWB/YYWzTVCAD5f270D/7uH07xZOxwirS2e7ExH3UtCLNFP7Csp57JUfKLXV8sffJ3D1\noNjTvr9tiD/jhvfg91d2J2PXYVZuyOWH9ANYLCYG9ookoXs4/btFENc+GLOuv4u0Ggp6kWboQKGN\nx19ZS0l5DXeN7cc1l8SdcVuTyUS/buH06xbOfeMGYDab8LLoTlqR1kpBL9LMHCyq4NH/XcuRshpu\nH92X6y7rcs6f1Zi33olIy6SgF2lkhmHw8bc7WbVxL107hpLUO5LE+HZYA3wabJt/+GjIHy6t5rbr\n+3DD0K5NULGIeDIFvUgjqrXX8/IHaaz+KQ+z2cS+Ahvfph593LtzG5J6RZLUO4pO7U4eAHfoSCWP\nvfIDRSVVTL+uN2Mv7+amvRART6KgF2kkxeXVPL14A1m5xcTHhPHorRdRaqthw9Z8Nm4tIHP3YTJ2\nHWbx8q1EtQ0gqXcUA3tF0q9rW0rKa3nslbUcOlLJlJE9+f2V3d29OyLiIRT0Io0g50Apc99YT1FJ\nFcMu6MR9Ewbg622hTbAfnTuEMOGqeErKa/gpq4CN2wrYvP0Qn3+/m8+/342fjwU/Hy9KbDVMujqe\nCVfFu3t3RMSDKOhFztO6/xzkL+/9RHVtPVNH9mLc8O6nvC89NMiX4UkxDE+KwV7nYGvOYTZuLWDj\n1nwOFFUw4aoeTLpaIS8ijUtBL3KODMPgw2928Pa/tuHrY+GR6UkM7t/hjNp6e5lJ6B5BQvcI/jC6\nL7Yq+2mnrRUROVcKepFzUGuv56UP0vj2pzzCQ/x4/LZBdO0Ues6fp5AXEVdR0IucpeKyalLe2sD2\nnwfdPXbrRYQF+7m7LBGRU1LQi5yF3ftLmffm0UF3lyd24r7xAzQpjYg0awp6kTO07j8HeeG9n6hp\nYNCdiEhzoqAXacDRme528dY/M/HxPrtBdyIi7uayoHc4HMyePZvs7Gy8vb1JSUkhJibmpPc98cQT\nhIaG8tBDDwEwduxYrFYrANHR0Tz99NOuKlGkQXX1Dl75aAsrfsylTbAfTyQPott5DLoTEWlqLgv6\nVatWYbfbWbp0Kenp6SxYsIBFixad8J6lS5eyY8cOLrroIgBqamoAWLJkiavKEjljtio7C/62gfQd\nRXTpGMKs5EG0DfF3d1kiImfFZWtXpqamMmTIEAASEhLIyMg4afuWLVuYMGEChmEAkJWVRVVVFcnJ\nyUyfPp309HRXlSdyWvmHK3j4r2tI31HEoD5RLPjjZQp5EWmRXHZGb7PZnF3wABaLBYfDgdls5tCh\nQyxcuJCFCxfyr3/9y/kef39/kpOTGTduHHv27OH2229nxYoVmM1aS1uazracIzy1eD1lFbWMGdaV\nW0b1wWLWoDsRaZlcFvRWq5WKigrn82MhD7BixQqKi4u5/fbbKSoqorq6mq5du3LttdcSGxsLQFxc\nHKGhoRQWFhIZGemqMkVO8F1qHi++v5l6h8E9N/Vn5ODO7i5JROS8uCzoExMTWb16NSNHjiQtLY34\n+F/m8J46dSpTp04F4OOPPyYnJ4cxY8bw3nvvsWPHDp588kkKCgqw2WxERES4qkQRJ8MwWLpyO++t\n3E6AnxePT0siMb6du8sSETlvDQb9qFGjGDNmDKNHjz6r0B0xYgRr165l4sSJAMyfP5/ly5dTWVnJ\n+PHjT9lm3LhxzJw5k5tvvhmTycT8+fPVbS9npa7ewZadRdTU1tE2xJ/wUH9Crb6YT9P1Xmuv56V/\npPFtah7t2gQwK3kQsVHBTVi1iIjrmIxjI+F+Q15eHp988gn//Oc/6dSpEzfeeCNXXXUV3t7un5s7\nLy+P4cOH8/XXX9OpUyd3lyNu4nAYbNtzhO9S8/h3+gHKK2tP2G4xm2gb4kfbEH8iQv1pG+pPeKgf\n4SH+hFh9+ds/t7JtzxHiY8N4/NZBhAb5umlPRERO71xyr8GgP95XX33FU089RXV1NTfccAP33HMP\nYWFh51zw+VLQt16GYbDnYBnfpebx3eb9FJVUAUeXgh06oCMRYQEcLq2isKSKwyVVFJVUcaSsGsdv\n/G0fMqAjD0y8AF9NZysizdi55F6DXfc2m40VK1bw6aefUlBQwKRJkxg5ciT//ve/SU5O5qOPPjrv\nwkXOVP7hCr7bnMd3qfvZV1AOQICfF1clxTAssSP9ukX85gj5+noHxeU1FJUeDf6ikmqKSqroGBHI\n7y6OO233vohIS9Vg0F911VVcfvnl3HfffQwcONA5t3d0dDRr1651eYHSulVU2ck5UMrOvBLWph8g\nK7cYOLqe++D+7Rl2QScG9oo8o4VlLBYz4aFHr9sT6+rKRUSahwaDftWqVeTm5tKnTx/Ky8vJyMjg\nkksuwWw2nzTTnci5MgyDgiOV5BwoZff+MnIOlJJzsIxDRyqd7zGbYECPCIZd0IlL+rUnUGu4i4g0\nqMGgf+WVV8jMzGTx4sVUVlaycOFCNm7cyP33398U9YmHqrHX8/3m/ezMKzka6gfKqKqpO+E9oVZf\nBvSIoEuHEDp3CKZ/9wjaaN13EZGz0mDQr169ms8++wyAyMhI3nrrLcaMGaOgl3PicBh8m5rHki+2\nOQfQmU3QsZ2Vzh1Cfv4VTJcOIYQp1EVEzluDQV9fX09VVZVzOtva2lqtwS3nZMvOQt78PJNdeaV4\ne5m56YpuDO7fgdj2wRrtLiLiIg0G/cSJE7npppu48sorMQyDNWvWMHny5KaoTTzEvoJy3lq+lQ1b\n8wG4PLETU0f2ol2bADdXJiLi+RoM+ltuuYXExEQ2bdqEl5cXzz//PL17926K2qSFKymv4b2VWaz4\nMReHw6BPl7Yk39CH7tHum3tBRKS1aTDoa2pqyM/Pp02bNhiGwdatW/nqq6944IEHmqI+aYFq7PV8\n+t0uPvxmB1U1dXSMCOTWUX24qE+ULvuIiDSxBoP+3nvvpbq6mtzcXJKSkti4cSMDBgxoitqkhXEO\ntPvXVopKqwkO9GH6tf343SVxeFm0ZoGIiDs0GPQ5OTnOqW9vuukm/vSnP2nEvZzEXlfPC++lsjb9\nAN5eZn5/ZXd+f2V33esuIuJmDQZ9eHg4JpOJLl26sH37dsaOHUttbW1DzaQVqay2k7J4A1t2FtG7\ncxsemnwh7cI00E5EpDloMOi7devGvHnzmDRpEjNmzODQoUPU1dU11ExaieLyama//iO795dycd8o\nZkwZqFvlRESakQaDfvbs2aSlpdGtWzfuu+8+1q1bxwsvvNAUtUkzd7CogidfW8fBwxX87uJY7r6x\nPxZdixcRaVYaDPpx48bx8ccfAzB8+HCGDx/u8qKk+duVV8Ls13+kxFbDhBE9mPy7nhpRLyLSDDV4\n+tW2bVs2btyo6/LilL6jkEcWraW0ooa7xvZjyjW9FPIiIs1Ug2f0GRkZTJ069YTXTCYT27Ztc1lR\n0nz9O30/L7ybCsCfpg7ksoSObq5IREROp8Gg//HHH5uiDmkB/rk2h1c/3oKfjxeP3XoRCd0j3F2S\niIg0oMGgf/nll0/5+r333tvoxUjzZBgG767I4v2vsgm1+vLk7RfTrVOou8sSEZEz0OA1esMwnI/t\ndjvffPMNhw8fdmlR0nzU1ztY+GE673+VTVTbAJ69b4hCXkSkBWnwjP6+++474fkf//hHbr31VpcV\nJM1HZbWd59/9iY1bC+jSMYTZt19MWJDWiBcRaUkaDPpfs9lsHDx40BW1SDNScKSSeW/8SG5+OQN6\nRPDI9CQC/DSdrYhIS9Ng0F955ZUnPC8tLSU5OdllBYn7Ze4+zNNvbaCsopZRl3XmDzf01UQ4IiIt\nVINB//bbb2MymTAMA5PJREhICFartSlqEzdYtSGXhR+m4zDgnpv6M3JwZ3eXJCIi56HB07SKigqe\ne+45OnXqRFVVFXfccQe7du1qitqkCdU7DN74LIMX30/Dz8eLuXdcopAXEfEADQb9448/ztixY4Gj\nC9z88Y8DpVLYAAAZQUlEQVR/5PHHH3d5YdJ0KqvtPPXmej75bhcdI6y88MBQ3SMvIuIhGgz66upq\nhg0b5nx+6aWXUlVV5dKipOnkH67g4Ze+Z9O2Ai7oEcHzDwylQ4QuzYiIeIoGgz4sLIz33nuPiooK\nbDYb//jHP2jbtm1T1CYulrn7MA+9uIa9+eVcP6QLT/7hYqz+GlkvIuJJGhyMN3/+fObMmcNzzz2H\nt7c3AwcOJCUlpSlqkzNUaqvhzc8zOVhUQWiQL2FBvoQG+RHmfOxLWJAfoUG++Py8VvxX63NZtCwd\nw4A//j6Bay6Jc+9OiIiISzQY9B07duSBBx6gT58+lJWVkZmZSVRUVFPUJmdg8/ZD/M/fUykur8Fk\nguMmMjylQD8vggN9OXi4gqAAb2ZOT6J/N12PFxHxVA0G/fPPP09mZiaLFy+murqaRYsWsXHjRu6/\n//6mqE9+g73OwTtfbOOjb3diMZu4dVRvRg/tiq3KTnF5DSXl1RSX11BcVkOJrYbi8mpKyn7+3VZD\nj5hQHpp8IR3CdT1eRMSTNRj0q1ev5rPPPgOgXbt2LF68mDFjxijo3ehAoY3n3tnEzrxSOoQHMmPK\nhXSPDgMgxOpLiNUX2ge7uUoREWkOGgz6+vp6qqqqnJPk1NbWYjKZXF6YnMwwDL7euI9XP95CdW09\nVyXFcMfYfvj7nvVMxiIi0ko0mBATJ07kpptu4sorr8QwDNasWcPkyZObojY5TkWVnUUfprMmbT8B\nfl48POVChl7Qyd1liYhIM9dg0E+aNAm73U5NTQ3BwcGMGzeOoqKipqhNfpa15wjPvfsTh45U0jM2\njBlTBhLZJsDdZYmISAvQYNDfe++9VFdXk5ubS1JSEhs3bmTAgAENfrDD4WD27NlkZ2fj7e1NSkoK\nMTExJ73viSeeIDQ0lIceeuiM27QW9Q6DD77O5u8rt4NhMGFEDyaNiNcCMyIicsYaTIycnBzefvtt\nRowYQXJyMh988AEFBQUNfvCqVauw2+0sXbqUGTNmsGDBgpPes3TpUnbs2OG85n8mbVqL6po6Zr36\nA+9+mUWbYD9S7r6UKdf0UsiLiMhZaTA1wsPDMZlMdOnShe3btxMZGUltbW2DH5yamsqQIUMASEhI\nICMj46TtW7ZsYcKECRg/3/zdUJvWwuEw+MvfU9mys4hBfaJ46aHL6ds13N1liYhIC9Rg0Hfr1o15\n8+YxaNAg/va3v/Hqq69SV1fX4AfbbLYTlrO1WCw4HA4ADh06xMKFC5k1a5Yz5Btq05q8tyKLdf85\nSL+u4cycnoQ1wMfdJYmISAvV4DX62bNnk5aWRrdu3bjvvvtYt24dL7zwQoMfbLVaqaiocD53OByY\nzUf/X7FixQqKi4u5/fbbKSoqorq6mi5dupy2TWvxXWoe76/Kpn3bQGZOT8JLXfUiInIeGgx6Ly8v\nBg4cCMDw4cMZPnz4GX1wYmIiq1evZuTIkaSlpREfH+/cNnXqVKZOnQrAxx9/TE5ODmPHjmXlypW/\n2aY1yN5bzIvvbybAz4snkgcRHKgzeREROT8um2llxIgRrF27lokTJwJHF8dZvnw5lZWVjB8//ozb\ntBaHS6tIWbye+noHj95yEdGRQe4uSUREPIDLgt5kMjFnzpwTXuvcufNJ7xs7duxp27QG1bV1PPXm\neo6U1ZB8Q18G9op0d0kiIuIhdAHYzRwOg/+3dDM780oZcVEMo4d2cXdJIiLiQRT0brb0q+2sTT9A\nny5tufumBK0jICIijUpB70bfp+3n7yu3065NAI9MT8LbS4dDREQal5LFTXbuK+H/Ld2Mv6+FWbcN\nOrq0rIiISCNT0LvB4dIq5r25HntdPTOmDCRWa8eLiIiLKOibWI29npTFGzhSVs0t1/Xhot5R7i5J\nREQ8mIK+CRmGwV+XbmbHvhKuHBjN2Mu7urskERHxcAr6JpS5+zBr0vbTK64N947TCHsREXE9BX0T\n2r2/FIBRl3XG28vi5mpERKQ1UNA3odz8cgBiozT4TkREmoaCvgnl5pdhMZvoEGFt+M0iIiKNQEHf\nRBwOg735ZXRsZ9XEOCIi0mSUOE2ksKSKqpp6dduLiEiTUtA3kdz8MgBio7T8rIiINB0FfRPJPXg0\n6GN0Ri8iIk1IQd9E9v484j5O092KiEgTUtA3kdz8Mny8LUS2CXB3KSIi0ooo6JtAfb2DfQU2YqKC\nMJs1G56IiDQdBX0TOFBUQV29QwPxRESkySnom8BezYgnIiJuoqBvAr/cWqegFxGRpqWgbwLOoG+v\nrnsREWlaCvomkHuwnEB/b9oE+7m7FBERaWUU9C5Wa6/nYJGN2KggrT8vIiJNTkHvYnmHbDgMXZ8X\nERH3UNC7mOa4FxERd1LQu5hzjntNfSsiIm6goHex3J/voY+J1Bm9iIg0PQW9i+3NLyMsyJcQq6+7\nSxERkVZIQe9CldV2DhVXaSCeiIi4jYLehfYW/Nxtr4lyRETETRT0LnRsIF6czuhFRMRNFPQudGwg\nXqxG3IuIiJso6F3o2Bl9tEbci4iImyjoXWhvfjmRbQLw9/VydykiItJKuSyBHA4Hs2fPJjs7G29v\nb1JSUoiJiXFuX7FiBa+//jomk4nrr7+eadOmATB27FisVisA0dHRPP30064q0aVKymsosdVwUe8o\nd5ciIiKtmMuCftWqVdjtdpYuXUp6ejoLFixg0aJFANTX1/OXv/yFZcuWERAQwLXXXssNN9yAv78/\nAEuWLHFVWU1mb4GWphUREfdzWdd9amoqQ4YMASAhIYGMjAznNovFwhdffIHVauXIkSM4HA68vb3J\nysqiqqqK5ORkpk+fTnp6uqvKc7ncgz/fWqcR9yIi4kYuC3qbzebsgoej4e5wOH75YrOZlStXMmbM\nGAYNGoS/vz/+/v4kJyfzxhtvMGfOHGbMmHFCm5ZEi9mIiEhz4LKgt1qtVFRUOJ87HA7M5hO/7uqr\nr+b777+ntraWTz75hLi4OG644QYA4uLiCA0NpbCw0FUlutTe/HLMZhOd2lkbfrOIiIiLuCzoExMT\nWbNmDQBpaWnEx8c7t9lsNqZOnUptbS0mkwl/f3/MZjPLli1jwYIFABQUFGCz2YiIiHBViS5jGAa5\n+WV0jAjE28vi7nJERKQVc9lgvBEjRrB27VomTpwIwPz581m+fDmVlZWMHz+e66+/nilTpuDl5UXP\nnj0ZPXo0dXV1zJw5k5tvvhmTycT8+fNP6gVoCYpKqqmsruOCeF2fFxER93JZ0JtMJubMmXPCa507\nd3Y+Hj9+POPHjz9hu7e3Ny+88IKrSmoyv1yfV9CLiIh7tbzT5RZgrwbiiYhIM6GgdwHNcS8iIs2F\ngt4FcvPL8PYyE9U20N2liIhIK6egb2T1DoN9+eVERwZhMZvcXY6IiLRyCvpGVnC4gto6B3HqthcR\nkWZAQd/INCOeiIg0Jwr6RnZsIJ7muBcRkeZAQd/Icg/qHnoREWk+FPSNLDe/jAA/L8JD/dxdioiI\niIK+Mdnr6tlfWEFsVDAmk0bci4iI+ynoG1HeIRsOh0GMBuKJiEgzoaBvRM4Z8XR9XkREmgkFfSNy\nznHfXmf0IiLSPCjoG1HuQZ3Ri4hI86Kgb0S5+WWEWn0Jsfq6uxQRERFAQd9oqmrqKDhSqYF4IiLS\nrCjoG8m+Ai1NKyIizY+CvpH8MiOezuhFRKT5UNA3Et1aJyIizZGCvpEcW7VO1+hFRKQ5UdA3kr35\nZbQL8yfAz9vdpYiIiDgp6BtBWUUtR8pqtDStiIg0Owr6RuCcEU/d9iIi0swo6BuBcyCebq0TEZFm\nRkHfCHKdZ/QKehERaV4U9I1gb345ZhN0amd1dykiIiInUNCfJ8MwyD1YRvtwKz7eFneXIyIicgIF\n/Xk6UlaNrcqupWlFRKRZUtCfJy1NKyIizZmC/jz9O30/AF06hri5EhERkZMp6M/DnoNlfL1xLzFR\nQST1inR3OSIiIidR0J+HNz/LwGHAraP6YLHoj1JERJofpdM5Ss06xObsQgb0iODCnu3cXY6IiMgp\nKejPQb3D4M3PMzCZ4Lbr+2AymdxdkoiIyCkp6M/Bqg17yc0v56qkGDp30CA8ERFpvhT0Z6mqpo53\nv9yGr4+Fydf0dHc5IiIip+Xlqg92OBzMnj2b7OxsvL29SUlJISYmxrl9xYoVvP7665hMJq6//nqm\nTZvWYJvmYNnqHRSX13Dz1fG0DfF3dzkiIiKn5bIz+lWrVmG321m6dCkzZsxgwYIFzm319fX85S9/\n4a233uL999/nvffeo7i4+LRtmoPDpVV8/O0u2gT7Mvbybu4uR0REpEEuO6NPTU1lyJAhACQkJJCR\nkeHcZrFY+OKLLzCbzRQVFeFwOPD29j5tm+ZgyRfbqLXXM2VsP/x8XfZHJyIi0mhcdkZvs9mwWn9Z\nzc1iseBwOH75YrOZlStXMmbMGAYNGkRAQECDbdxp9/5Svtm0j7j2wVyZ1LwuJ4iIiPwWlwW91Wql\noqLC+dzhcGA2n/h1V199Nd9//z21tbV88sknZ9TGHQzD4I3PMjCMo7fTWcy6nU5ERFoGl6VoYmIi\na9asASAtLY34+HjnNpvNxtSpU6mtrcVkMuHv74/ZbD5tG3fatK2ALTuLuLBnOy6I1+Q4IiLScrjs\nQvOIESNYu3YtEydOBGD+/PksX76cyspKxo8fz/XXX8+UKVPw8vKiZ8+ejB49GuCkNu5WX+9g8fJM\nzCa49fo+7i5HRETkrLgs6E0mE3PmzDnhtc6dOzsfjx8/nvHjx5/U7tdt3G3l+lz2Fdj43cWxWopW\nRERaHPdfAG/GKqvtvLsiC39fC5N/p8lxRESk5VHQn8aH3+yg1FbLTVd0JyzYz93liIiInDUF/W8o\nLK7i0+920TbEj9HDurq7HBERkXOioP8Nb3+xldo6B1NH9sLPR5PjiIhIy6SgP4Ud+4r59qc8unQM\n4YoLo91djoiIyDlT0P/KgSIbTy/eAEDyDX0wa3IcERFpwRT0xzlQZOOxRWspKq3m1lF96N8twt0l\niYiInBcF/c9+HfI3XqHV6UREpOVT0KOQFxERz9Xqg14hLyIinqxV3zd2oMjGo4vWclghLyIiHqrV\nBv3xIX/b9X0Ye7lCXkREPE+r7LpXyIuISGvR6oJeIS8iIq1Jqwp6hbyIiLQ2HnuN3jAMamrrKaus\npbyiliNl1Sz8MF0hLyIirYpHBP3b/9qKyfcA5RV2yitrKauopbyyFnud46T3KuRFRKQ18YigX7N5\nP94BVQAE+nkRFOhDXPtgggJ9CA7wISjQh6AAH7pHhzKwV6SbqxUREWk6HhH08+4cTPeucQQFeGOx\ntKphByIiIqflEUHfPjyQ0CBfd5chIiLS7Oj0V0RExIMp6EVERDyYgl5ERMSDKehFREQ8mIJeRETE\ngynoRUREPJiCXkRExIMp6EVERDyYgl5ERMSDKehFREQ8mIJeRETEgynoRUREPJiCXkRExIMp6EVE\nRDyYgl5ERMSDKehFREQ8mJerPtjhcDB79myys7Px9vYmJSWFmJgY5/bly5fz9ttvY7FY6NGjB7Nn\nz8ZkMjF27FisVisA0dHRPP30064qUURExOO5LOhXrVqF3W5n6dKlpKens2DBAhYtWgRAdXU1L774\nIsuXL8fX15eHHnqI1atXc+mllwKwZMkSV5UlIiLSqris6z41NZUhQ4YAkJCQQEZGhnObr68v77//\nPr6+vgDU1dXh5+dHVlYWVVVVJCcnM336dNLT011VnoiISKvgsjN6m83m7IIHsFgsOBwOzGYzJpOJ\nNm3aAEfP3quqqhg8eDDZ2dkkJyczbtw49uzZw+23386KFSswm0/9/5H6+noA8vPzXbUbIiIizcax\nvDuWf2fCZUFvtVqpqKhwPj8W8sc/f+6558jNzeWll14CIC4ujtjYWOfj0NBQCgsLiYyMPOV3FBYW\nAjB58mRX7YaIiEizU1hY6MzLhrgs6BMTE1m9ejUjR44kLS2N+Pj4E7bPmjULX19fFi5ciMlkAmDZ\nsmVkZ2fz5JNPUlBQgM1mIyIi4je/o2/fvrz77rtERERgsVhctSsiIiLNQn19PYWFhfTt2/eM25gM\nwzBcUYxhGMyePZvt27cDMH/+fDIzM6msrKRv377cdNNNDBw40Pn+6dOnM2zYMGbOnMnBgwcxmUw8\n/PDDDBgwwBXliYiItAouC3oRERFxP02YIyIi4sEU9CIiIh5MQS8iIuLBFPQiIiIezGW317laQ3Pp\nt3SeOud/eno6zz//PEuWLCE3N5eZM2diNpvp3r07Tz75pPNWy5bq+P3bunUrd911l/Ne10mTJnHt\ntde6ucJzY7fbefTRRzlw4AC1tbXcfffddO3a1WOO36n2LyoqijvvvJO4uDigZR+/+vp6Hn/8cfbs\n2YPJZGLOnDn4+Ph4xPE71b7Z7XaPOXbHHD58mBtvvJG33noLs9l8dsfOaKFWrFhhzJw50zAMw0hL\nSzPuvvtuN1fUeKqrq40xY8a4u4xG99prrxmjRo0yJkyYYBiGYdx5553Ghg0bDMMwjFmzZhlfffWV\nO8s7b7/ev3/84x/Gm2++6eaqGseyZcuMp59+2jAMwygpKTGGDRtm3HXXXR5z/E61f550/L766ivj\n0UcfNQzDMNavX2/cddddHnP8fr1vd999t0cdO8MwjNraWuOee+4xfve73xm7du0665+dLbbr/nRz\n6bd0njrnf2xsLC+//DLGz3d0bt26laSkJACGDh3KDz/84M7yztuv9y8jI4Nvv/2WKVOm8Nhjj50w\nU2RLc80113D//fcDR3vTvLy8POr4nWr/MjMzPeb4XXXVVcydOxeA/fv3ExISQmZmpkccv1/vW3Bw\nsEcdO4Bnn32WSZMmOSeQO9t/ey026H9rLn1P4O/vT3JyMm+88QZz5sxhxowZHrFvV1999QkzGBrH\nTeEQEBBAeXm5O8pqNL/ev4SEBP785z/zzjvvEB0dzcsvv+zG6s5PQEAAgYGB2Gw2HnjgAf7rv/7r\nhL+TLf34/Xr/HnzwQfr37+8xxw+O/oz885//TEpKCtdff71H/fv79b550rH76KOPaNOmDZdddhlw\n9Ofm2R67Fhv0Dc2l35LFxcVxww03OB8fm/Pf0xx/vCoqKggODnZjNY1vxIgR9O7dGzh61rFt2zY3\nV3R+Dh48yPTp0xkzZgyjRo3yuON3/P5dd911Hnf8AJ555hm+/PJLHn/8cWpra52ve8LxO7ZvTzzx\nBJdeeqnHHLuPPvqIH374galTp5KVlcXMmTMpLi52bj+TY9dikzExMZE1a9YAnHIu/ZZs2bJlLFiw\nAOCM5vxvqXr16sWGDRsAWLNmzQlTInuC5ORktmzZAsC6devOam7q5qaoqIjbbruNhx9+mBtvvBHw\nrON3qv3zpOP36aef8tprrwHg5+eH2Wymb9++HnH8fr1vJpOJ++67z2OO3TvvvMOSJUtYsmQJPXv2\n5JlnnuGyyy47q2PXYqfANU4xl37nzp3dXFXjsNvtHjvnf15eHjNmzGDp0qXs2bOHJ554ArvdTteu\nXXnqqada5Kjf4x2/f1u3bmXevHl4eXnRrl075s6dS2BgoLtLPCdPPfUUX3755Qn/xh577DFSUlI8\n4vidav/++7//m2effdYjjl9VVRWPPPIIRUVF1NXVcccdd9ClSxeP+Pd3qn2LiorymH97x5s6dSpz\n587FZDKd1bFrsUEvIiIiDWuxXfciIiLSMAW9iIiIB1PQi4iIeDAFvYiIiAdT0IuIiHgwBb2IiIgH\nU9CLiMt89NFHPPLII+4uQ6RVU9CLiMu0xAlYRDxNi12PXkQaz2uvvcaXX35JfX09l112GRMnTuSe\ne+4hJiaG3NxcOnTowHPPPUdISAirV6/mxRdfxOFwEB0dzdy5c2nbti0//PADzzzzDA6Hg44dO/L8\n889jGAa5ublMnTqVgwcPcskllzBv3jx3765Iq6IzepFWbs2aNWRmZvLhhx/y8ccfU1BQwOeff86O\nHTu45ZZbWL58OV27duWll17i8OHDPPnkkyxatIjPPvuMxMRE5s6dS21tLQ8//DDPPPMMn3/+OfHx\n8XzyySeYTCYOHjzIwoUL+de//sWaNWvYtWuXu3dZpFXRGb1IK7du3Tq2bNniXMylpqYGwzDo3Lmz\nc83rMWPGMGPGDC677DL69+9Phw4dAJgwYQKvvfYa2dnZREZG0rNnTwAefPBB4Og1+oEDBzpX14qJ\niTlh5S0RcT0FvUgr53A4mD59OrfccgsA5eXl5OfnO8P62HssFssJa9Afe72urg4vrxN/lNhsNmw2\nGyaT6aRtWl5DpGmp616klbv44ov59NNPqayspK6ujnvuuYeMjAxycnLIysoCji6dPGzYMBISEkhL\nS2P//v0AvP/++1x88cV06dKFI0eOOLvlX3/9dZYuXeq2fRKRX+iMXqSVu+KKK8jKymL8+PHU19cz\ndOhQkpKSCAkJ4a9//St79+4lPj6eGTNm4Ofnx7x587j33nux2+107NiRlJQUfHx8eO655/jTn/6E\n3W4nNjaWZ599li+//NLduyfS6mmZWhE5SV5eHtOmTeObb75xdykicp7UdS8ip6R74EU8g87oRURE\nPJjO6EVERDyYgl5ERMSDKehFREQ8mIJeRETEgynoRUREPNj/B8AopkkCSuDvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b154f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here is a visualization of the training process\n",
    "# typically we gain a lot in the beginning and then\n",
    "# training slows down\n",
    "plt.plot(history.history['acc'])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tune an existing CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TODO - Break the compilation of the notebook on purpose as the following is just a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This script goes along the blog post\n",
    "\"Building powerful image classification models using very little data\"\n",
    "from blog.keras.io.\n",
    "It uses data that can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "In our setup, we:\n",
    "- created a data/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "'''\n",
    "\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = 'fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'cats_and_dogs_small/train'\n",
    "validation_data_dir = 'cats_and_dogs_small/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# build the VGG16 network\n",
    "model = applications.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "model.add(top_model)\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
